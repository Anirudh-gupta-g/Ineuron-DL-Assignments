{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e1c6b2",
   "metadata": {},
   "source": [
    "**1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
    "\n",
    "Ans:\n",
    "\n",
    "Is it OK to initialize the bias terms to 0?\n",
    "Name three advantages of the SELU activation function over ReLU.\n",
    "In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax? 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer? 6. Name three ways you can produce a sparse model. 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout? 8. Practice training a deep neural network on the CIFAR10 image dataset: a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function. b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters. c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed? d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.). e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "**2. Is it OK to initialize the bias terms to 0?**\n",
    "\n",
    "Ans: Yes, it is generally acceptable to initialize the bias terms to 0. However, it is important to note that other values may result in better performance, depending on the type of model and data.\n",
    "\n",
    "**3. Name three advantages of the SELU activation function over ReLU.**\n",
    "\n",
    "Ans:\n",
    "\n",
    "SELU is self-normalizing, meaning it does not suffer from the \"dying ReLU\" problem.\n",
    "\n",
    "SELU has a higher mean and variance than ReLU, which can help to reduce internal covariate shift and can lead to faster training.\n",
    "\n",
    "SELU is more robust to outliers and can maintain better performance under noisy data.\n",
    "\n",
    "**4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
    "\n",
    "Ans:\n",
    "\n",
    "SELU: When training deep neural networks and when dealing with noisy data or outliers.\n",
    "\n",
    "Leaky ReLU (and its variants): When training deep neural networks and when dealing with data that is not linearly separable.\n",
    "\n",
    "ReLU: When training deep neural networks and when dealing with data that is linearly separable.\n",
    "\n",
    "Tanh: When training shallow neural networks and when dealing with data that is non-linear.\n",
    "\n",
    "Logistic: When training shallow neural networks and when dealing with binary classification tasks.\n",
    "\n",
    "Softmax: When training shallow neural networks and when dealing with multi-class classification tasks.\n",
    "\n",
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?**\n",
    "\n",
    "Ans: If the momentum hyperparameter is set too close to 1, the SGD optimizer may cause oscillations in the optimization trajectory and can lead to slow convergence or even divergence of the optimization process. This is because the SGD optimizer will attempt to move too quickly in the direction of the previous update, resulting in overshooting the optimum.\n",
    "\n",
    "**6. Name three ways you can produce a sparse model.**\n",
    "\n",
    "Ans:\n",
    "\n",
    "Use L1 regularization, which adds a penalty on the sum of the absolute values of the weights in the model. This encourages the model to reduce the number of non-zero weights, leading to a sparse model.\n",
    "\n",
    "Use feature selection techniques such as forward selection, backward selection, or recursive feature elimination to select only the most relevant features in the model. This can reduce the number of inputs, leading to a sparse model.\n",
    "\n",
    "Use pruning techniques such as magnitude pruning or low-rank factorization to remove redundant weights from the model. This can lead to a more efficient and sparse model.\n",
    "\n",
    "**7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**\n",
    "\n",
    "Ans: Dropout does slow down training, as it requires more iterations for the model to converge. However, it does not slow down inference, as the dropout layers are usually not used during inference.\n",
    "\n",
    "MC Dropout does slow down inference, as it requires multiple forward passes and additional computations to sample multiple weights from the dropout layers.\n",
    "\n",
    "**8. Practice training a deep neural network on the CIFAR10 image dataset:**\n",
    "\n",
    "**a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.**\n",
    "\n",
    "**b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.****\n",
    "\n",
    "**c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?**\n",
    "\n",
    "**d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).**\n",
    "\n",
    "**e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.**\n",
    "\n",
    "Ans:\n",
    "\n",
    "**a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fcae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Downloading tensorflow_intel-2.12.0-cp39-cp39-win_amd64.whl (272.8 MB)\n",
      "     ------------------------------------ 272.8/272.8 MB 587.2 kB/s eta 0:00:00\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 795.3 kB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     -------------------------------------- 57.5/57.5 kB 377.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.3.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (60.10.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ------------------------------------ 126.5/126.5 kB 297.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "     ------------------------------------ 440.7/440.7 kB 834.2 kB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.3-cp39-cp39-win_amd64.whl (422 kB)\n",
      "     ------------------------------------ 422.5/422.5 kB 849.9 kB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 660.4 kB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp39-cp39-win_amd64.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 772.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.23.5-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     ---------------------------------------- 14.7/14.7 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 599.5 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.9.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (4.11.3)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl (938 kB)\n",
      "     ------------------------------------ 938.4/938.4 kB 715.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "     ------------------------------------ 182.1/182.1 kB 203.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.11)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.6->jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ------------------------------------ 151.7/151.7 kB 900.9 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518835 sha256=1ee2ebb1e28a2043e36ac9165f4474367b5e8f767412fc52cca8fe87513c72cf\n",
      "  Stored in directory: c:\\users\\aniru\\appdata\\local\\pip\\cache\\wheels\\ac\\c9\\8c\\f4c803770fde18dcdd82e84675eb6add1c0d1035f7214a96fa\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, opt-einsum, ml-dtypes, google-auth, jax, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/keras/\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\aniru\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff00a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Using cached tensorflow_intel-2.12.0-cp39-cp39-win_amd64.whl (272.8 MB)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.56.0-cp39-cp39-win_amd64.whl (4.2 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.13-py3-none-any.whl\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (60.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.9.1)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Using cached ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl (938 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.1)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.11)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.6->jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\aniru\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: opt-einsum, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 gast-0.4.0 google-auth-2.21.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 jax-0.4.13 keras-2.12.0 ml-dtypes-0.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-intel-2.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c87a54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec83dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 262s 2us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18710232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a3173e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "591b2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa76116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ad1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d827b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 21s 9ms/step - loss: 2.0070 - accuracy: 0.2645 - val_loss: 1.8359 - val_accuracy: 0.3296\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.8239 - accuracy: 0.3360 - val_loss: 1.7601 - val_accuracy: 0.3656\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7621 - accuracy: 0.3624 - val_loss: 1.7569 - val_accuracy: 0.3588\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.7126 - accuracy: 0.3854 - val_loss: 1.7094 - val_accuracy: 0.3789\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.6748 - accuracy: 0.4013 - val_loss: 1.6582 - val_accuracy: 0.4062\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.7048 - accuracy: 0.3893 - val_loss: 1.6429 - val_accuracy: 0.4185\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.6337 - accuracy: 0.4188 - val_loss: 1.6294 - val_accuracy: 0.4174\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.6129 - accuracy: 0.4273 - val_loss: 1.6879 - val_accuracy: 0.4106\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.5922 - accuracy: 0.4349 - val_loss: 1.6354 - val_accuracy: 0.4192\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.5796 - accuracy: 0.4389 - val_loss: 1.6857 - val_accuracy: 0.4104\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.7237 - accuracy: 0.3766 - val_loss: 1.7889 - val_accuracy: 0.3668\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6171 - accuracy: 0.4240 - val_loss: 1.9108 - val_accuracy: 0.2898\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.7095 - accuracy: 0.3807 - val_loss: 1.6609 - val_accuracy: 0.4069\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5976 - accuracy: 0.4295 - val_loss: 1.5883 - val_accuracy: 0.4275\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5631 - accuracy: 0.4437 - val_loss: 1.5804 - val_accuracy: 0.4339\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5420 - accuracy: 0.4526 - val_loss: 1.5802 - val_accuracy: 0.4364\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5225 - accuracy: 0.4610 - val_loss: 1.5487 - val_accuracy: 0.4456\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.5028 - accuracy: 0.4685 - val_loss: 1.5511 - val_accuracy: 0.4610\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 632.6288 - accuracy: 0.4325 - val_loss: 1.9989 - val_accuracy: 0.2656\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.7915 - accuracy: 0.3524 - val_loss: 1.7310 - val_accuracy: 0.3826\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.6522 - accuracy: 0.4051 - val_loss: 1.6710 - val_accuracy: 0.4033\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6331 - accuracy: 0.4100 - val_loss: 1.6333 - val_accuracy: 0.4177\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.5888 - accuracy: 0.4311 - val_loss: 1.5974 - val_accuracy: 0.4277\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.5633 - accuracy: 0.4376 - val_loss: 1.5642 - val_accuracy: 0.4402\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.5484 - accuracy: 0.4452 - val_loss: 1.5989 - val_accuracy: 0.4314\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.5330 - accuracy: 0.4504 - val_loss: 1.5999 - val_accuracy: 0.4374\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.5238 - accuracy: 0.4544 - val_loss: 1.5690 - val_accuracy: 0.4433\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.5093 - accuracy: 0.4611 - val_loss: 1.5609 - val_accuracy: 0.4487\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.4988 - accuracy: 0.4640 - val_loss: 1.5556 - val_accuracy: 0.4462\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.4944 - accuracy: 0.4659 - val_loss: 1.5868 - val_accuracy: 0.4357\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef09c9",
   "metadata": {},
   "source": [
    "**b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.**\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2de1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99b1125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb34fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pixel values to float and normalize\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5912f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c9723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d26cdfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f72692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74b68d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.4566 - accuracy: 0.4775 - val_loss: 1.2252 - val_accuracy: 0.5668\n",
      "Epoch 2/50\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1235 - accuracy: 0.6065 - val_loss: 1.0778 - val_accuracy: 0.6252\n",
      "Epoch 3/50\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 0.9885 - accuracy: 0.6557 - val_loss: 1.0474 - val_accuracy: 0.6408\n",
      "Epoch 4/50\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.8990 - accuracy: 0.6872 - val_loss: 0.9141 - val_accuracy: 0.6816\n",
      "Epoch 5/50\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.8318 - accuracy: 0.7117 - val_loss: 0.8860 - val_accuracy: 0.6968\n",
      "Epoch 6/50\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.7681 - accuracy: 0.7349 - val_loss: 0.8724 - val_accuracy: 0.6990\n",
      "Epoch 7/50\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.7152 - accuracy: 0.7509 - val_loss: 0.8882 - val_accuracy: 0.6956\n",
      "Epoch 8/50\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.6708 - accuracy: 0.7684 - val_loss: 0.8688 - val_accuracy: 0.7072\n",
      "Epoch 9/50\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.6271 - accuracy: 0.7829 - val_loss: 0.8864 - val_accuracy: 0.7078\n",
      "Epoch 10/50\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.5876 - accuracy: 0.7944 - val_loss: 0.9323 - val_accuracy: 0.6976\n",
      "Epoch 11/50\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 0.5498 - accuracy: 0.8084 - val_loss: 0.8996 - val_accuracy: 0.7114\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35a3c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6978999972343445\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23832a",
   "metadata": {},
   "source": [
    "**c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bcb9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c3a0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19701f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pixel values to float and normalize\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55fbf672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f767ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture with Batch Normalization\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eccfdd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c89dc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "137438b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1407/1407 [==============================] - 46s 31ms/step - loss: 1.2693 - accuracy: 0.5563 - val_loss: 1.5615 - val_accuracy: 0.5108\n",
      "Epoch 2/50\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 0.9410 - accuracy: 0.6730 - val_loss: 1.0333 - val_accuracy: 0.6526\n",
      "Epoch 3/50\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 0.8175 - accuracy: 0.7160 - val_loss: 1.0635 - val_accuracy: 0.6472\n",
      "Epoch 4/50\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 0.7182 - accuracy: 0.7516 - val_loss: 1.2648 - val_accuracy: 0.5836\n",
      "Epoch 5/50\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 0.6343 - accuracy: 0.7772 - val_loss: 0.8285 - val_accuracy: 0.7318\n",
      "Epoch 6/50\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 0.5600 - accuracy: 0.8047 - val_loss: 1.2513 - val_accuracy: 0.6124\n",
      "Epoch 7/50\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.5017 - accuracy: 0.8255 - val_loss: 1.1357 - val_accuracy: 0.6470\n",
      "Epoch 8/50\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 0.4420 - accuracy: 0.8456 - val_loss: 1.3755 - val_accuracy: 0.6286\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_bn = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d577878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Batch Normalization: 0.7210000157356262\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss_bn, test_acc_bn = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy with Batch Normalization:\", test_acc_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fda2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "126a14ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAACEy0lEQVR4nO3dd3gUVffA8e+m9xBCKoQEQg299y4dxI6oCIKvIkoVX/WnvgoWFBUBERQVUFFARBBFKQKB0EMJvbdACqGmkbqZ3x/DLlmSQMoms5ucz/Pss7OzszNnN4E9uffce3WKoigIIYQQQlQgNloHIIQQQghR1iQBEkIIIUSFIwmQEEIIISocSYCEEEIIUeFIAiSEEEKICkcSICGEEEJUOJIACSGEEKLCkQRICCGEEBWOJEBCCCGEqHAkARLCijz88MM4Oztz8+bNAo95+umnsbe35/Lly4U+r06n47333jM+Dg8PR6fTER4eft/XDh8+nJCQkEJfK7c5c+awcOHCPPvPnz+PTqfL97myNHHiRHQ6HQMGDNA0DiGE+UkCJIQVGTlyJOnp6fzyyy/5Pp+YmMiKFSsYMGAAfn5+xb5O8+bN2bFjB82bNy/2OQqjoAQoICCAHTt20L9//1K9/r1kZWWxaNEiANasWUNMTIxmsQghzE8SICGsSN++fQkMDGT+/Pn5Pr948WLS0tIYOXJkia7j4eFB27Zt8fDwKNF5isvR0ZG2bdvi4+OjyfUB/vjjD65cuUL//v3R6/X88MMPmsVyP7du3dI6BCGsjiRAQlgRW1tbhg0bxt69ezl06FCe5xcsWEBAQAB9+/blypUrjB49mrCwMNzc3PD19aV79+5ERETc9zoFdYEtXLiQunXr4ujoSP369fnxxx/zff3kyZNp06YNlStXxsPDg+bNm/P999+Te+3lkJAQjhw5wubNm9HpdOh0OmNXWkFdYFu3bqVHjx64u7vj4uJC+/btWb16dZ4YdTodmzZt4qWXXqJKlSp4e3vzyCOPEBsbe9/3bvD999/j4ODAggULCAoKYsGCBeS3dvTx48cZMmQIfn5+ODo6Ur16dZ599lkyMjKMx8TExPDCCy8QFBSEg4MDgYGBPPbYY8ZuSkPM58+fNzl3fj+Hrl270rBhQ7Zs2UL79u1xcXFhxIgRACxdupRevXoREBCAs7Mz9evX54033iA1NTVP3Lt27WLgwIF4e3vj5OREaGgo48ePByAiIgKdTsfixYvzvO7HH39Ep9MRGRlZ6M9SCEskCZAQVmbEiBHodLo8rUBHjx5l9+7dDBs2DFtbW65fvw7Au+++y+rVq1mwYAE1a9aka9euhartudvChQt57rnnqF+/PsuXL+ftt9/m/fffZ+PGjXmOPX/+PC+++CK//vorv//+O4888ghjxozh/fffNx6zYsUKatasSbNmzdixYwc7duxgxYoVBV5/8+bNdO/encTERL7//nsWL16Mu7s7AwcOZOnSpXmOf/7557G3t+eXX35h2rRphIeH88wzzxTqvV66dIl169YxaNAgfHx8GDZsGKdPn2bLli0mxx04cIBWrVqxc+dOpkyZwj///MPUqVPJyMggMzMTUJOfVq1asWLFCiZOnMg///zDjBkz8PT05MaNG4WK525xcXE888wzPPXUU/z999+MHj0agFOnTtGvXz++//571qxZw/jx4/n1118ZOHCgyevXrl1Lp06diI6OZvr06fzzzz+8/fbbxoSsU6dONGvWjK+++irPtWfPnk2rVq1o1apVsWIXwmIoQgir06VLF6VKlSpKZmamcd+rr76qAMrJkyfzfU12draSlZWl9OjRQ3n44YdNngOUd9991/h406ZNCqBs2rRJURRF0ev1SmBgoNK8eXMlJyfHeNz58+cVe3t7JTg4uMBY9Xq9kpWVpUyZMkXx9vY2eX2DBg2ULl265HnNuXPnFEBZsGCBcV/btm0VX19fJTk52eQ9NWzYUKlWrZrxvAsWLFAAZfTo0SbnnDZtmgIocXFxBcZqMGXKFAVQ1qxZoyiKopw9e1bR6XTK0KFDTY7r3r27UqlSJSUhIaHAc40YMUKxt7dXjh49WuAxhpjPnTtnsv/un4OiqD97QNmwYcM930NOTo6SlZWlbN68WQGUAwcOGJ8LDQ1VQkNDlbS0tPvGtH//fuO+3bt3K4Dyww8/3PPaQlgDaQESwgqNHDmSq1evsmrVKgCys7NZtGgRnTp1onbt2sbjvv76a5o3b46TkxN2dnbY29uzYcMGjh07VqTrnThxgtjYWJ566il0Op1xf3BwMO3bt89z/MaNG3nggQfw9PTE1tYWe3t7/ve//3Ht2jUSEhKK/H5TU1PZtWsXjz32GG5ubsb9tra2DB06lEuXLnHixAmT1zz44IMmjxs3bgzAhQsX7nktRVGM3V49e/YEoEaNGnTt2pXly5eTlJQEqHU3mzdv5oknnrhnrdI///xDt27dqF+/fuHf8H14eXnRvXv3PPvPnj3LU089hb+/v/Fz79KlC4DxZ37y5EnOnDnDyJEjcXJyKvAaQ4YMwdfX16QV6Msvv8THx4fBgweb7b0IoRVJgISwQo899hienp4sWLAAgL///pvLly+bFD9Pnz6dl156iTZt2rB8+XJ27txJZGQkffr0IS0trUjXu3btGgD+/v55nrt73+7du+nVqxcA3377Ldu2bSMyMpK33noLoMjXBrhx4waKohAQEJDnucDAQJMYDby9vU0eOzo6Fur6Gzdu5Ny5czz++OMkJSVx8+ZNbt68yRNPPMGtW7eMdTE3btxAr9dTrVq1e57vypUr9z2mqPL7HFJSUujUqRO7du3igw8+IDw8nMjISH7//Xfgzvu+cuUKwH1jcnR05MUXX+SXX37h5s2bXLlyhV9//ZXnn3/e+FkKYc3stA5ACFF0zs7ODBkyhG+//Za4uDjmz5+Pu7s7jz/+uPGYRYsW0bVrV+bOnWvy2uTk5CJfz5BMxMfH53nu7n1LlizB3t6ev/76y6SFYeXKlUW+roGXlxc2NjbExcXlec5Q2FylSpVinz+377//HlATyOnTp+f7/IsvvkjlypWxtbXl0qVL9zyfj4/PfY8xfE65C6cBrl69mu/xuVvhDDZu3EhsbCzh4eHGVh8gz5xRhtaq+8UE8NJLL/Hxxx8zf/580tPTyc7OZtSoUfd9nRDWQFqAhLBSI0eORK/X8+mnn/L333/z5JNP4uLiYnxep9Pl+Uv94MGD7Nixo8jXqlu3LgEBASxevNhkJNSFCxfYvn27ybE6nQ47OztsbW2N+9LS0vjpp5/ynNfR0bFQLUKurq60adOG33//3eT4nJwcFi1aRLVq1ahTp06R39fdbty4wYoVK+jQoQObNm3Kc3v66aeJjIzk8OHDODs706VLF5YtW1ZgogLq1AWbNm3K00WXm2H028GDB032G7o4C8OQFN39M//mm29MHtepU4fQ0FDmz5+fJ+G6W0BAAI8//jhz5szh66+/ZuDAgVSvXr3QMQlhySQBEsJKtWzZksaNGzNjxgyysrLyzP0zYMAA1q1bx7vvvsvGjRuZO3cuvXv3pkaNGkW+lo2NDe+//z579+7l4YcfZvXq1fz888888MADebrA+vfvT0pKCk899RTr169nyZIldOrUKd9uk0aNGnHgwAGWLl1KZGRkvkP7DaZOncq1a9fo1q0bv/32G6tWraJfv34cPnyYzz77LN9WkaL6+eefSU9PZ+zYsXTt2jXP7fXXXwdMW4mysrJo06YN3377LZs2bWLJkiU89dRTxpa2KVOmUKVKFTp37szMmTPZuHEjv//+Oy+88ALHjx8HoFWrVtStW5dJkyaxePFi1qxZw4svvsjWrVsLHXv79u3x8vJi1KhRrFixgr/++oshQ4Zw4MCBPMd+9dVXXLhwgbZt2/Ljjz8SHh7Ojz/+yNNPP53n2HHjxnHmzBkuXrzIK6+8UuTPVAiLpXERthCiBGbOnKkASlhYWJ7nMjIylEmTJilVq1ZVnJyclObNmysrV65Uhg0blmfUFvcZBWbw3XffKbVr11YcHByUOnXqKPPnz8/3fPPnz1fq1q2rODo6KjVr1lSmTp2qfP/993lGOp0/f17p1auX4u7urgDG8+Q3CkxRFCUiIkLp3r274urqqjg7Oytt27ZV/vzzT5NjDKOXIiMjTfYX9J5ya9q0qeLr66tkZGQUeEzbtm2VKlWqGI85evSo8vjjjyve3t6Kg4ODUr16dWX48OFKenq68TUXL15URowYofj7+yv29vZKYGCg8sQTTyiXL182HnPy5EmlV69eioeHh+Lj46OMGTNGWb16db6jwBo0aJBvbNu3b1fatWunuLi4KD4+Psrzzz+v7Nu3L9/PcseOHUrfvn0VT09PxdHRUQkNDVUmTJiQ73lDQkKU+vXrF/iZCGGNdIqSz8xeQgghBGq3XJMmTfjqq6+M8w0JUR5IAiSEECKPM2fOcOHCBf7v//6P6OhoTp8+bVJjJoS1kxogIYQQebz//vv07NmTlJQUli1bJsmPKHekBUgIIYQQFY60AAkhhBCiwpEESAghhBAVjiRAQgghhKhwZCmMfOTk5BAbG4u7u7tZJlcTQgghROlTFIXk5GQCAwOxsbl3G48kQPmIjY0lKChI6zCEEEIIUQwXL16874K/kgDlw93dHVA/QA8PD42jEUIIIURhJCUlERQUZPwevxdJgPJh6Pby8PCQBEgIIYSwMoUpX5EiaCGEEEJUOJIACSGEEKLCkQRICCGEEBWOJEBCCCGEqHAkARJCCCFEhSMJkBBCCCEqHEmAhBBCCFHhSAIkhBBCiApHEiAhhBBCVDiSAAkhhBCiwpEESAghhBAVjiRAQgghhKhwJAESQhSfooA+W+sohBCiyCQBEkIU3/KR8FltSIrVOhIhhCgSSYCEEMWTnQnH/oS063ByjdbRCCFEkUgCJIQonoSjoM9Ut89v1TYWIYQoIkmAhBDFExd1Z/v8VrUeSAghrIQkQEKI4ondf2c75TJcO6NdLEIIUUSSAAkhiic2Sr23sVPvz0doFooQQhSVJEBCiKLLzlRrgAAaPqbeX9imXTxCCFFEkgAJIYrOUADtVAmaPqXukzogIYQVkQRICFF0hvqfgCYQ1BpsHSA5Dq6f1TYuIYQoJEmAhBBFZxgBFtgM7J2hakv1sQyHF0JYCUmAhBBFZyiADmyq3od0VO8lARJCWAlJgIQQRZO7ADqgqXof0kG9v7BN6oCEEFZBEiAhRNHkLoD2ClH3VWsNNvaQFAM3zmkZnRBCFIokQEKIojEUQAc2BZ1O3XZwgaot1O3zMhxeCGH5JAESQhSNoQDa0P1lIHVAQggrIgmQEKJo7i6ANjDUAcl8QEIIKyAJkBCi8LIz4PIRdfvuFqCgNuqyGEmX4OaFMg9NCCGKQhIgIUThJRyFnCzTAmgDB9dcdUDSDSaEsGySAAkhCi9395ehADq3YEM3mBRCCyEsmyRAQojCK6gA2kAKoYUQVkISICFE4RVUAG0Q1AZ0tpAYDTekDkgIYbkkARJCFM69CqANHN2ganN1+4J0gwkhLJckQEKIwrlXAXRu0g0mhLACkgAJIQrnfgXQBsGSAAkhLJ8kQEKIwrlfAbRB9dt1QDcvwM2LpR2VEEIUiyRAQojCuV8BtIGj+51jpA5ICGGhJAESQtxf7gLowGb3P95YBxRRejEJIUQJSAIkhLi/3AXQlYLvf3xIJ/Ve6oCEEBZKEiAhxP0VtgDaIKgN6GzgxnlIvFSKgQkhRPFIAiSEuL/CFkAbOHncOVaWxRBCWCBJgIQQ9xe7X70vTP2PQcjtdcEuSDeYEMLUxeu3OH81VdMYJAESQtxbdgZcPqpu328EWG5SBySEuMuxuCTGL9lP18/CmfrPMU1jsdP06kIIy1fUAmiD6m3VOqDrZyEpFjwCSy1EIYTlUhSFXeeu8/XmM4SfuGLcn5aVQ5Y+B3tbbdpiJAESQtxbUQugDZw8wb+xWj90fhs0frwUghNCWKqcHIV1Ry/z9eYzRF28CYCNDvo1CmBUl1AaVvXUND5JgIQQ91bUAujcQjreToAiJAESooLIyNbzx/5YvtlyhjNX1DofBzsbHm9RjRc61yTY21XjCFWSAAkh7q04BdAGIR1hx2yZEVqICiA5PYvFu6P5fus5LidlAODhZMfQdsEMb18DH3dHjSM0JQmQEKJgxS2ANqjeDtDBtdOQHA/u/uaMTpRTOTkKWTk5ZOsVsvV3trP0as1Ido66na1XyM7JITNbvTccY3g+S6+Qrc8hK0e9N5wr6/bxhufvHJ+DDh2ta1SmVwM/3J3stf4orMKV5AwWbDvHTzsvkJyeDYCfhyPPd6zJkDbVcXO0zFTDMqMSQliG4hZAGzhXAv9GEH9QHQ3W6DFzRygsUOKtLFZGxbD55BXSMvVqcnFXEnInocmbvOhzFE3jX7rnIg4rbOhW14eBTQLpUc8PZwdbTWOyROevpjIv4iy/7b1EZnYOADV9XBnVOZRBzQJxtLPsz0wSICFEwYpbAJ1bSCdJgCoARVHYefY6SyOj+ftwvPEL0VzsbHTY29pgZ3v7/l6Pbe7st7fVYWdz13F2Ntjb6LDL5/jUjGzWHonnzJVU1h65zNojl3FxsKVnmB8DGwfSqU4Vi/9iL22HYxKZu/kM/xyKw5CrNqteiVFdQulZ3w8bm2L+X1HGJAESQhSsJPU/BiEdYedXMh9QOZWQlM5v+y7xa+RFzl+7Zdxfz9+dx1pUw9fDCYd8khA7WxscjAmM6fN3JzJ2Njp0xU3Ai+G13nU5FpfMnwdj+fNALJdupPFHVCx/RMXi4WRHn4b+DGwSSLua3thpNIS7rCmKwrbT1/h68xm2nr5q3N+trg+juoTSukblMv0ZmYMkQEKIgpVkBJhBsKEO6BQkXwZ3PzMEJrSUrc9hy6krLN59kY3HE4xdVq4OtjzYtCpDWgfRqKqn1X0hGuh0OsICPQgL9OC/vesSdfEmfx6I46+DsSQkZ/Drnkv8uucS3q4O9GsUwMAmgbQM9rKalo+i0Oco/HM4jq83n+FwTBIAtjY6HmwSyItdalLP30PjCItPpyiKtp2tFigpKQlPT08SExPx8LDeH64QJZKdAR9VVWuAxh0Ar5Din+vrjhB/CB6bDw0fNVuIomxdvH6LX/dcZNmeS8QnpRv3twj2YnCrIPo3CsDVQgtezUGfoxB5/jp/Hojl70Nx3LiVZXzO38OJAY3VZKhxNetN/gzSs/T8tvcS30ac5cLtlj0nexuebFWdkR1rEFTZReMI81eU7+/y+5sqhCiZkhZA5xZ8OwE6v00SICuTka1n3ZHLLI28aNL14eViz6PNqzG4VRC1/dw1jLDs2NroaFvTm7Y1vXnvwQZsP3ONPw/EsvZwPPFJ6Xy39RzfbT1H9couDGyiJkN1/dytKhlKTMti0c4LLNh2jqspmQBUcrFnWLsQhrUPobKrg8YRmo8kQEKI/BkLoJsVvwDaIKQj7JordUBW5OTlZJZGXuT3fZdMWjo61a7C4FZB9Azzq9DFwPa2NnSp40OXOj588FBDtpy8wp8H4/j36GWir9/iq01n+GrTGWr7ujGwSSADmwRSo4plTACYn/jEdOZvO8cvu6JJyVCHslet5MzznWowuFUQLg7lL10of+9ICGEexgLopiU/V3B79f7qCUhJADffkp9TmF1qRjarD8axJDKafdE3jfv9PZx4omU1Hm8ZZLFdH1pysrelVwN/ejXw51ZmNhuOJfDngVjCT1zhVEIK09efZPr6kzSs6sGDTQLp3ziQqpWctQ4bgNMJKczbcoYV+2PI0qsVMXX93HmxS00GNgnUbJ2usqB5AjRnzhw+/fRT4uLiaNCgATNmzKBTp075Hjt8+HB++OGHPPvDwsI4cuSI8fHy5ct55513OHPmDKGhoXz44Yc8/PDDpfYehCiXzFEAbeBSGfwawuXD6qzQDeTfo6VQFIUDlxJZGhnNqqhYUjP1gNrd06OeL0NaV6dzHR9sy2GBb2lwcbAztvgkpmWx/uhl/jwQy9bTVzkck8ThmCQ++vs4LYO9GNgkkL6N/PF1dyrzOPdF3+Dr8DOsP3YZQyVw65DKjOpak251fa2q2664NC2CXrp0KUOHDmXOnDl06NCBb775hu+++46jR49SvXr1PMcnJiaSlpZmfJydnU2TJk0YM2YM7733HgA7duygU6dOvP/++zz88MOsWLGC//3vf2zdupU2bdoUKi4pghYVnjkLoA3+eR12fQ2tnof+n5f8fKJEbt7KZOX+GJZEXuR4fLJxf4i3C4NbVefRFlU1+WIur66lZLDmSDyromLZff66Memw0UG7UG8GNg6kT0N/KrmUXo2NoiiEn7jC3M1n2H3uunF/zzA/RnUJpUWwV6ldu6wU5ftb0wSoTZs2NG/enLlz5xr31a9fn4ceeoipU6fe9/UrV67kkUce4dy5cwQHq0WagwcPJikpiX/++cd4XJ8+ffDy8mLx4sWFiksSIFHhxe6HeV3VAujXz5e8Bgjg6Cr4dSj41IeXd5b8fKLIcnIUdp67xtLIi/yTa7JCRzsb+jUKYHCrINpY4Xwu1iY+MZ3Vh+L480CscZV0UCd77FzHh4FNAugZ5m+2JSSy9Tn8dVAdym5Idu1tdTzUtCovdqlJLd/yU8RuFaPAMjMz2bt3L2+88YbJ/l69erF9+/ZCneP777/ngQceMCY/oLYATZgwweS43r17M2PGjALPk5GRQUZGhvFxUlJSoa4vRLmVewJEc30ZBndQ768cg9Sr4FrFPOcV95WQlM6yvZf4dc9F45BmgPoBHgxpHcSgJlXxdJF1r8qKv6cTIzvWYGTHGly8fuv2hItxHItLYuPxBDYeT8DR7hDd6/kysEkg3ev54mRf9ILzW5nZ/Bp5kW8jzhFzU+09cXWw5ak21RnRsQYBnpZRh6QVzRKgq1evotfr8fMznRTNz8+P+Pj4+74+Li6Of/75h19++cVkf3x8fJHPOXXqVCZPnlyE6IUo53IvgWEurt7gG6YOr7+wDcIGme/c96EoCikZ2dy8lcWNW5lk6XPw83DCz8Op3BZ5Zutz2HzyCksiTScrdHO048GmgTzZyronKywvgiq7MLprLUZ3rcXphGT+PKC2DJ29mso/h+P553A8roalOJoE0qm2Dw529/6dvZGayQ87zvPD9vPGEXzerg481yGEoW1DJNm9TfMi6Lv/8SmKUqh/kAsXLqRSpUo89NBDJT7nm2++ycSJE42Pk5KSCAoKum8MQpRb5iyAzi2ko5oAnd9a7AQoS5/DzVtZ3LyVyY3bCY3Jdqphn3p/41YWiWmZxhEuuel04OvuSGAlZwI9nQnwdFK3KzkR4OlMYCVnvF0drGqG3+hrtycr3HuRy0l3WrZbGiYrbBxQLoc0lwe1fN2Z0NOd8Q/U5mhckjEZirmZxsqoWFZGxeLpbE+fBupSHG1rVjZZiuPSjVt8F3GOpZEXSctSi9mrV3bhP51r8niLasVqRSrPNPtXUKVKFWxtbfO0zCQkJORpwbmboijMnz+foUOH4uBgWjDm7+9f5HM6Ojri6OhYxHcgRDmVnQGXj6rb5mwBAjUB2j0Pzm9FURRSM/XcSM2drNzZzp3A3DQ8l5pF8u05SorD0c4GLxcH7Gx1JCRlkKnP4XJSBpeTMtjPzXxf42BrQ0AlJzU5up0UBVQy3fZw0vYv6vQsPeuOXmZpZDTbTl8z7q/s6sAjzaryZOugclXnUd7pdDoaBHrSINCT1/vUZf/Fm/x5IJbVB+NISM5g6Z6LLN1zkSpu6lIcnWr78M+hOP44EGts6QsL8OClrqH0behfYdYrKyrNi6BbtGjBnDlzjPvCwsIYNGjQPYugw8PD6datG4cOHaJhw4Ymzw0ePJjk5GT+/vtv476+fftSqVIlKYIWojBi9sG33cDZC/57rlA1QNn6HBLTsoytMKZJzZ0EJjv5Ct8nPAlAm6x5XNa7FStEnQ48nOzxcrGnkosDXi72eLk4GLcruebep957uTjg7HDnL+CcHIWrqRnE3UwnLjGNmJvpxN1MIy4xnZibacQlppGQnEFh/od0c7TL1WqkJkcBlZwJvN2i5O/pVCp/fZ+IT2ZJZDQr9sdw83ZXh04HHWtV4clW1XkgzLdCT1ZY3uhzFHafu86fB2P5566lOAzah3ozqksonWpXqZDdm1ZRBA0wceJEhg4dSsuWLWnXrh3z5s0jOjqaUaNGAWrXVExMDD/++KPJ677//nvatGmTJ/kBGDduHJ07d+aTTz5h0KBB/PHHH/z7779s3Soz0ApRKLm7v+7xH+jR2CRmbTjF9jNXSUovfKvMCYdq1LW5RFPlKGtpjYOdTZ5kxTSpuZ3AuBqSHQc8ne1LPC+NjY0OX3cnfN2daBJUKd9jMrNzuJyUTlxiOrE304hNTCPupmFbTZxu3soiJSObk5dTOHk5pcDrVXFzIKCAbrbASmochXlPqRnZ/HUwliWRF9mfa7LCAE8nHm8ZxOMtqslkheWUrY2OdqHetAv1ZvKDDdh2+ip/Hohj59lrNAnyZFSXUBpXq6R1mFZD0wRo8ODBXLt2jSlTphAXF0fDhg35+++/jaO64uLiiI6ONnlNYmIiy5cvZ+bMmfmes3379ixZsoS3336bd955h9DQUJYuXVroOYCEqPDuUwB96nIyM/49xepDcXme83S2p1KeVhlDC4y63/1wVzi1iM9bp/BFv94429ta7F+qDnY2BFV2uWdCcSszm9jbSVFcYlqu7XRiE9OIvZlGelYOV1MyuZqSyaGYxHzPY2ujw99D7WoLqJSrJel2wpSRrWfZnkv8eeDOZIV2NjoeqO/H4NZBdK4tkxVWJPa2NnSt60vXujKrenHJavD5kC4wUaF90xniDsDjP0CDh4y7z11NZea/J/njQCyKojYODWgcyH861aCal0vhW2WOrIBlw9WZoV/aVmpvw1IoisLNW1m3kyFDd1uasest9mY68UnpxtqNwqhRxZXBrYJ4pLlMVihEblbTBSaEsDAmBdDNALh4/RZfbjzF8n0xxi/p3g38mNCzDvX8i/EHQnBH9f7yYbh1XV0moxzT6XR4uTrg5epAg0DPfI/R5yhcSc4w1h7F3bxTh2TofkvL1NO7gT+DWwXRWiYrFKLEJAESQtxx+Yi6/IWzF3E6H2avOMSvey4ah5B3r+fLxJ51aFg1/y/yQnHzgSp11YVRL2yH+gPMFLz1srXR4e/phL+nE2D9yxEIYQ0kARJC3HG7APqMXS36frbZuFRCx1pVmNirDs2rm+nLOaSjmgCd3yoJkBBCE5IACSEAuJ6ayYVd4TQD1t4IIDM7h9Y1KvNqzzq0qelt3ouFdIA938MFGZ0phNCGJEBCVHCJt7L4NuIsC7adYwkHwAZSKjdk0cA2dKjlXTq1JoY6oPjDkHZDnXNICCHKkCRAQlRQyelZLNh2nm8jzpKcno0DWdRzugTAa88NRudViouVuvuBd224dgou7IB6/UrvWkIIkQ9JgISoYG5lZvPD9gt8s+WMcfbgun7uvNsiA/uN2eDsha5ScOkHEtJRTYDOb5UESAhR5iQBEqKCSM/Ss2jnBb7efIarKZkA1PRxZfwDdRjQKACbfQvUA+8zA7TZhHSEvQvgfETpX0sIIe4iCZAQ5VxGtp5fIy8ye9Np4+rg1Su7MK5HbQY1DbyzUOJ9ZoA2uxBDHdAhSLsJzpXK5rpCCIEkQEKUW1n6HJbvvcSXG08TczMNgEBPJ8b2qM2jLaphf/cK0YY1wG5PgFjq3P3BuxZcOw3RO6Bu37K5rhBCIAmQEOWOPkfhj6gYZm44xYVrtwDwdXfkle61GNwqKP/VwXPPAB3QtOyCDe6gJkDnt0oCJIQoU5IACVFO5OQorD4Ux4x/T3LmSioA3q4OvNQ1lGfaBuNkn0/iY5BrBmgqVS+jiIGQTrDvBzUBEkKIMiQJkBBWTlEU1h29zBfrT3I8PhlQV2V/sUtNhrULwdWxEP/MDd1fZVUAbRDSQb2PPwjpieBUgiU2hBCiCCQBEsJKKYpC+IkrTF9/kkMxiQC4O9rxfKeajOgYgruTfeFPZiyALqP6HwOPQKhcE66fheidUKd32V5fCFFhSQIkhJVRFIVtp68xff0J9kXfBMDFwZbnOoTwn041qeTiUPSTxu5X78tqBFhuIR3VBOh8hCRAQogyIwmQEFZk97nrfL7uBLvOXQfAyd6GZ9uF8GLnmni7ORbvpNkZkHBM3S7LAmiD4I6w70c4v63sry2EqLAkARLCCuyPvsH09SeJOHUVAAdbG55qU53RXUPx9XAq2cm1KoA2MNQBxUVBehI4eZR9DEKICkcSICEs2OGYRL5Yf5INxxMAsLPR8USrIF7pVovASs7muYhWBdAGntXAKwRunIeLu6B2z7KPQQhR4UgCJIQFOhGfzBfrT7LmSDwANjp4tHk1xvaoTVBlF/NezFj/U8YF0LmFdFQToPMRkgAJIcqEJEBCWJAzV1KY8e8p/joYi6KoDTIPNglkXI/a1PRxK52LlvUSGPkJ6QT7F8l8QEKIMiMJkBAWQFEUPv7nON9GnCVHUff1a+TP+AfqUMfPvfQurHUBtEHw7Tqg2CjISAbHUnzPQgiBJEBCWIRley/xzZazADxQ35cJPevQILAMJgXUugDaoFIQVAqGmxcgehfUfkC7WIQQFYLN/Q8RQpSmM1dSePePIwC81rsu3w1rVTbJD5gugKpFAXRuhtXhL0g3mBCi9EkCJISGMrL1jPllP2lZejrU8ualLqFlG4ChAFrL7i8DQwIkdUBCiDIgCZAQGvrknxMcjUuisqsD059oio1NGbfCWEIBtIGhDihmH2SkaBuLEKLckwRICI1sPH6Z+dvOAfDZ443xK+mEhkVlKQXQBl7B4FkdFL06H5AQQpQiSYCE0EBCUjqTlh0E4LkOIXSv51f2QRgLoCtrWwCdm3SDCSHKiCRAQpSxnByFCb9GcT01k7AAD97oW0+bQHIvgKp1AbSBYVmMC7IumBCidEkCJEQZ+3rLGbadvoazvS2zhjTD0c5Wm0ByL4FhKQwtQDF7ITNV21iEEOWaJEBClKF90Tf4fN1JACY/2IBavqU0u3NhWFIBtEGlYPAMgpxsqQMSQpQqSYCEKCNJ6VmMW7IffY7CgMYBPN6ymnbBZKVbVgG0gU53ZzTYeekGE0KUHkmAhCgDiqLw1orDXLyeRjUvZz56pBE6LetuEiywANpACqGFEGVAEiAhysBvey/x54FYbG10zBrSDA8ne20Dyt39ZSkF0AaGQuiYvZB5S9tYhBDlliRAQpSyM1dSeHeVutTFxJ51aF7dS+OIsMwCaAOvGuBRVW2hurRb62iEEOWUJEBClKKMbD1jF+/nVqae9qHejCrrpS4KYokF0AZSBySEKAOSAAlRiqatOcGR2CS8XOz5YnBTbMt6qYv85C6ADmymbSwFkTogIUQpkwRIiFKy6XgC3281LHXRpOyXuihI7gJozyCto8mfcT6gPZCVpm0sQohySRIgIUqButTFAQCGtw+hR30NlrooiCUXQBtUrgnuAaDPhEuRWkcjhCiHJAESwswMS11cS82kvpZLXRTEkgugDXQ66QYTQpQqSYCEMLNvtpw1LnXx5ZBmONlrtNRFQYwtQBZa/2MghdBCiFIkCZAQZrQ/+gafrzsBwHsPhmm71EV+stIh4ai6bYkjwHIL6aTeX4pU4xZCCDOSBEgIM0lKz2Lskv1k5yj0bxzAEy0tsMA44Yi6zpYlF0AbeIeCmz/oM6QOSAhhdpIACWEGiqLwdu6lLh7WeKmLglhDAbSBTndnVugL0g0mhDAvSYCEMIPl+2JYdXupi5lPNsPTWeOlLgpiDQXQuUkhtBCilEgCJEQJnb2Swv/+OAyoS120CLaApS4KYi0F0AbBtxMgqQMSQpiZJEBClEBGtp4xt5e6aFfTgpa6yI81FUAbVKkNrr6Qna4ujiqEEGYiCZAQJfCpJS51URBrKoA2kDogIUQpkQRIiGLadCKB724vdfHpY03w97SQpS4KYk0F0LkZ64AitI1DCFGuSAIkRDEkJKUz6dc7S108EGZBS10UxFAAbS31PwaG+YAu7obsDG1jEUKUG5IACVFEOTkKE389wLXUTOr5u1veUhcFid2v3lvLCDCDKnXA1ed2HdA+raMRQpQTkgAJUUTzIs6y9fRVnOxtmP2UBS51kZ+sdEg4pm5bSwG0gU6Xa1kMGQ4vhDAPSYCEKIKoizf5bO3tpS4GNqCWr7vGERWSNRZA52aoA7ogCZAQwjwkARKikJLTsxi7+PZSF40CGNzKihKJ3PP/WFMBtIEhAYreBdmZ2sYihCgXJAESohAUReHtlYeJvn6LqpWc+egRC13qoiCG+h9r6/4y8KkHLt6QnQaxUgckhCg5SYCEKITf98XwR5S61MWsIU0td6mLgljbEhh3kzogIYSZSQIkxH2cvZLCO7eXupjwQG1aBFfWOKIisuYC6NwMw+ElARJCmIEkQELcQ2Z2DmOXqEtdtK1ZmZe61tI6pKIzFEC7eFtnAbSBYUboi7tAn6VtLEIIqycJkBD3MG3NcQ7HJFHJxZ4Zg5tZ9lIXBTEUQAc0tc4CaAOf+uootqxbd2qahBCimCQBEqIAVrfURUGsvQDawMbmTiuQdIMJIUpIEiAh8pGQfGepi2HtgulpDUtdFMTaC6BzCzasCyYJkBCiZCQBEuIuOTkKr+Za6uLNfvW1Dqn4yksBtIFxPqCdUgckhCgRSYCEuMu3EWeJOKUudfHlECtZ6qIg5aUA2sA3DJy9ICsV4g5oHY0QwopJAiRELgcu3uTT20tdvDuwAbX9rGSpi4LkXgDVmgugDWxscs0HFKFtLEIIqyYJkBC3JadnMXbJnaUunrSmpS4KYlwCo6mWUZiXMQHapm0cQgirZqd1AMLCKQqEfwyJF8GvAfg1BP9G4GJlkwEWwv/+OMKFa1a61EVBylMBtIGxDmgH6LPBVv4bE0IUneYtQHPmzKFGjRo4OTnRokULIiLu3aydkZHBW2+9RXBwMI6OjoSGhjJ//nzj8wsXLkSn0+W5paenl/ZbKZ8uH4bNH0PUz7D2/+DHB2FaDfi8Hix6DP59Dw79BgnH1S8jK/X7vkus2B9jvUtd5MekALqZtrGYk19DcKoEmSlSBySEKDZN/3RaunQp48ePZ86cOXTo0IFvvvmGvn37cvToUapXr57va5544gkuX77M999/T61atUhISCA72/SL18PDgxMnTpjsc3Ky0jlctHZpj3rvVUNtAYo/BDcvQHKceju9/s6xto7gWw/8GqnH+jdUv6wsvLXo3NVU3lmpLnUxvocVLnVREJMC6GpaR2M+NjYQ3B5O/A0XtkK1FlpHJISwQpomQNOnT2fkyJE8//zzAMyYMYO1a9cyd+5cpk6dmuf4NWvWsHnzZs6ePUvlyuqXVEhISJ7jdDod/v7+pRp7hWFYebvBw/DAu+p2ehIkHFVbh+IPw+Uj6s0wMufuv8rdA28nQ7m60CqHWkTXRWZ2DmMX7yc1U0+bGpUZ3c0Kl7ooSHkrgM4tpKOaAJ3fCh3GaR2NEMIKafYNlJmZyd69e3njjTdM9vfq1Yvt27fn+5pVq1bRsmVLpk2bxk8//YSrqysPPvgg77//Ps7OzsbjUlJSCA4ORq/X07RpU95//32aNSu4CyAjI4OMjAzj46SkpBK+u3Ik5nYCVLX5nX1OHlC9rXozyMmBG+duJ0OGxOjw7daiWPV2at2d4+2cwKfe7YTodkuRX4Myby36dO1xDsUkqktdPNnUOpe6KEh5LIA2MBRCR++UOiAhRLFo9r/G1atX0ev1+PmZzrDr5+dHfHx8vq85e/YsW7duxcnJiRUrVnD16lVGjx7N9evXjXVA9erVY+HChTRq1IikpCRmzpxJhw4dOHDgALVr1873vFOnTmXy5MnmfYPlQWbqnRqSqvfpZrCxAe9Q9Rb24J39htai+ENqQnT5CFw+eru1KOpOka6BR9VcLUW3E6NSai0KP5HAtxHqUhfTHm1MgKfzfV5hZQyfbXmq/zHwbwSOnpCRCPEHTRN0IYQoBM3/bLp7pI2iKAWOvsnJyUGn0/Hzzz/j6ekJqN1ojz32GF999RXOzs60bduWtm3vtEx06NCB5s2b8+WXXzJr1qx8z/vmm28yceJE4+OkpCSCgsrBEOiSijsIih7c/MEjsHjnuGdrUe4utENwMxqSYtTb3a1FvvVvJ0a56oucvYr91hKS05m0TO2qe7ZdML0alLMu09wF0OVpBJiBja1aB3TyH7UbTBIgIUQRaZYAValSBVtb2zytPQkJCXlahQwCAgKoWrWqMfkBqF+/PoqicOnSpXxbeGxsbGjVqhWnTp0qMBZHR0ccHR2L+U7KMUP9z/1af4rKpLVo0J396Ylq69Dlw3eSo4Sjd1b/vnsFcI9qpsXWfg3Vc9rce+Zmw1IXV1PUpS7+z5qXuijI5XJaAJ1bSAc1AbqwDTqM1ToaIYSV0SwBcnBwoEWLFqxfv56HH37YuH/9+vUMGjQo39d06NCBZcuWkZKSgpubGwAnT57ExsaGatXy/09eURSioqJo1KiR+d9EeRezV72vWkZdKE6eENxOvRkYWoviD92pL7p8+HZr0SX1dmrtneONrUUNoV5/qNs3z2W+21qOlrooSFw5LoA2MMwHdGE75Ojvm/haNUVRR10WtyVWCJGHpl1gEydOZOjQobRs2ZJ27doxb948oqOjGTVqFKB2TcXExPDjjz8C8NRTT/H+++/z3HPPMXnyZK5evcprr73GiBEjjEXQkydPpm3bttSuXZukpCRmzZpFVFQUX331lWbv02rFlFILUFHkbi1q8NCd/emJd0afGZKju1uLon6GV/aor73twMWbTFtTjpa6KEh5LoA28G8Mjh6QkaT+DpTX95qjh1+fheN/Qf2B0OcT8KyqdVRCWD1NE6DBgwdz7do1pkyZQlxcHA0bNuTvv/8mODgYgLi4OKKjo43Hu7m5sX79esaMGUPLli3x9vbmiSee4IMPPjAec/PmTV544QXi4+Px9PSkWbNmbNmyhdatW5f5+7Nqt66rLS9gmUW0Tp5qDUhw+zv7cvRw47z6ZbhjNlyKhO2zYOBMAFIyso1LXfRr5F8+lrooSHkugDawsYXq7dQWwPNby28CtPYtNfkBOPYnnNkE3d6C1i/I6DchSkCnKIqidRCWJikpCU9PTxITE/Hw8NA6HG2c/hcWPaqOwBq7T+toiu7CDljQB2wdYPwhcPdn4tIoft8fQ9VKzvw9thOeLuVgtuf8ZKXD1KpqDdD4w1CpHCd622bC+v9B3X4wZLHW0Zjfrnnwz2vqds/31QTo0m71cUATGPCFti20QliYonx/a74UhrBQ+c3/Y02C20FQG9Bnws65/L7vEr/vj8FGBzOfbFp+kx+oGAXQBsY6oG1qC2B5cnItrHld3X7gPbXQe8RatUXTyVOdcPTbHvD3a2qXsBCiSCQBEvkzFkBb8V+XHScAkBP5PZ+sVP9qHv9AHVqGlJOlLgpSEQqgDfybgIP77Zqww1pHYz5xB2HZc6DkQLOh0GG8ut/GBloMh1f2QuPBgAK758Hs1nBkhVosLYQoFEmARF6KYhkF0CVVuzc5Vephk5nMw/q1tK5RmZfL01IXBTEWQJfj+h8DW7s7c0yd36ZtLOaSFAu/DFYnC63RRe3mujuRdfOBR+bBs3+o3dQp8bBsOPz8OFw/p0nYQlgbSYBEXomXIDUBbOzUGXetlY0Nq90fB2Ck/RpmPlavfC11URBjAXRTLaMoO4ZusPNbtY3DHDJS1OQnOVZdKuaJH8H2Ht21NbvCS9uhyxtqvdvp9TCnLUR8DtmZZRa2ENZIEiCRl2ECRN8wsLfe5SE2n7zCxGO1iVG88eEmAedWah1S6SvvM0Dnx6QOKEfbWEoiRw/Ln1eX9nCpAk8tBedK93+dvRN0exNe2gE1OkN2OmyYAt90UgcDCCHyJQmQyMtY/2OlBdDAleQMXv01iizsOFD1aXXn9lnlr1D2bhWpANogoAk4uEH6TUg4onU0xbf2LXVmazsnGLIEvEKK9voqteDZVfDwPDWBunJcHQn5xyvqtBZCCBOSAIm8rLz+58K1VF7+eR9XUzKp6+dO96cngVMluH5WHUZcnhkKoAOblf8CaANb+1x1QFbaDbZrHuyaq24//A0EtSreeXQ6aDIYXomE5sPUfft/gtktIeoXKZIWIhdJgISpnJxcRbTW1QJ0OSmdt1Ycosfnm9l9/rq61MVTzXBy9YQ2L6oHbf2ifH8JGH52FaX7yyC4g3pvjQnQ3cPdc894XlwuleHBWeqwed8wuHUNVr4EPwyEKydLfn4hyoEiJ0AhISFMmTLFZIZmUY5cOwWZyWDvohZhWoEbqZlM/fsYnadt4udd0WTnKHSp48OK0R2oY1jqovWLYOesFgif26xpvKWqIiyBkZ+QTuq9tdUBFTTc3Vyqt4UXt8ADk9Xf//MRMLc9bPwQstLMey0hrEyRE6BXX32VP/74g5o1a9KzZ0+WLFlCRkZGacQmtGCo/wloYvHT7KdmZPPlhlN0nraJb7acJSM7h5bBXix9oS0/jGhN/YBcs4C6ekPzZ9XtrTM0ibfUZaXDlQpWAG0Q2BTsXSHtxp3PwNIVZri7OdjaQ8fx8PIuqN0bcrJgyzSY0w7ObDT/9YSwEkVOgMaMGcPevXvZu3cvYWFhjB07loCAAF555RX27bPCJROEKSuo/0nP0jN/6zk6T9vE5+tPkpyRTf0AD+YPb8myUe1oU9M7/xe2exl0tnB2k7pYanlTEQugDWztoXobddsausFyD3evUvf+w93NwStYHVn2xI/gHqCu9ffTw/DbSEi+XLrXFsICFbsGqEmTJsycOZOYmBjeffddvvvuO1q1akWTJk2YP38+ssSYlTK0AFngJHrZ+hx+jbxI98/CmfLXUa6lZhLi7cKsIc1YPaYj3ev5obvXX9BewdDwUXV728yyCbosVcQC6NysZT6gu4e7P/1r4Ya7m4NOB2GD4OXd0OYl0NnA4d9gdiuI/N66ug+FKKFi93FkZWWxYsUKFixYwPr162nbti0jR44kNjaWt956i3///ZdffvnFnLGK0padoa6kDhbVApSTo7DmSDyfrzvBmSupAPh7ODHugdo81qIa9rZFyOM7jodDv8LRP+DaGfAOLZ2gtVBRC6ANgu+aD8jGQsd4lHS4uzk4eUDfj9URY3+OV2vjVk9UR4oNnGHdE6AKUUhFToD27dvHggULWLx4Mba2tgwdOpQvvviCevXuFMz26tWLzp07mzVQUQYuH1brA5wra/Of8l0URWHLqat8uvY4h2OSAPBysWd011oMbReMk71t0U/q1wBq94JT62D7l+p/9uVFRS2ANghsphbv37qmzoHjF6Z1RHmZa7i7uQQ2g/9shMjvYMP7ELMHvukCbV+Crm+Co5u28QlRior8J1KrVq04deoUc+fO5dKlS3z22WcmyQ9AWFgYTz75pNmCFGUk9wrwGneh7L1wncHzdjJs/m4OxyTh6mDLuB612fLfbvync83iJT8GtxdJJeqX8lP7UJELoA3sHCCotbp9wQLXBSuN4e7mYGOrThPxym4IewgUPeyYDV+1geN/ax2dEKWmyC1AZ8+eJTg4+J7HuLq6smDBgmIHJTRiAQXQx+KS+GztCTYcTwDAwc6GoW2DGd01FG83R/NcpHo7qNYaLu1W/xp/4D3znFdLxgLoKhWvADq3kI5wNlwd7t36P1pHc0dpD3c3B49AeOIHOLkO/n4VbkbDkiFQtz/0m1axf69EuVTkFqCEhAR27dqVZ/+uXbvYs2ePWYISGjEWQJf9BIjnr6YydvF++s2KYMPxBGxtdDzZKojwSV15Z0CY+ZIfUFu3Oo5XtyO/h/RE851bK8YC6Kaat95pylAHdH6b5Ux4WVbD3c2lTi8YvUttKbWxgxOrYXZr2D4b9NlaRyeE2RQ5AXr55Ze5ePFinv0xMTG8/PLLZglKaCA9Ca7eniG2DNcAi09M583fD9Fj+mZWHYhFUWBA4wDWT+jMx482JrBSKS3GWqevOvw4Iwn2lIPWSsOw/ora/WVQtbk64d+tq3DlhNbRaDPc3RwcXNSW0RcjIKitmrytewu+7QqX9modnRBmUeQE6OjRozRvnvcLslmzZhw9etQsQQkNxEUBCngGgZtvqV/uRmomH/19jC6fbmLx7mj0OQpd6/rw15iOzH6qOTV9Srn40sYGOoxTt3fOVUfAWbPYA+p9RS2ANrBzvFMHdD5C21hy9LB8pDbD3c3FLwye+wce/FJdTy/+EHzXA1a/Wj5aTkWFVuQEyNHRkcuX8xaOxsXFYWdn2TMHi3vIXQBdilIyspn57yk6TdvEvNuzN7cK8eLXF9ux8LnWNKzqWarXN9HocfCoCinxcGBJ2V3X3HIXQFvg/E1lLiTXcHgtrf0/OLlG2+Hu5mBjo86iPmYvNBkCKOqosdmt4NBvltPVKEQRFTkB6tmzJ2+++SaJiXey/5s3b/J///d/9OzZ06zBiTJUyvU/6Vl6vos4S+dpm/ji35OkZGQTFuDBguda8euL7Whdo3KpXPee7BzU2aFBnRgxR1/2MZhD7gJoj6paR6O93BMiavXlvGse7Ppa3baE4e7m4FoFHv4ahv0J3rUg5bLawrXoEbh+VuvohCiyIjfZfP7553Tu3Jng4GCaNVP/2oyKisLPz4+ffvrJ7AGKMmKoITHzCLBsfQ7L911i5r+niE1MB6BGFVcm9qxD/0YB2NhoXAzafBhsngbXz8Dxv9RZcq1N7O3Wu4peAG1QtYXa6pJ6Ba6eAp86ZXv93MPde7xrOcPdzaVGZ3hpu7qmXsTn6npic9pB50nQfpz6h4UQVqDILUBVq1bl4MGDTJs2jbCwMFq0aMHMmTM5dOgQQUFBpRGjKG0pCZB4EdCZrYYkJ0fhr4Ox9PpiC68vP0RsYjoBnk58/Egj1k3ozMAmgdonP6BO9Nb6BXV76wzrbM6Pi1LvK3oBtIGdI1S73eJS1nVAdw93N8w5Vd7YOULX12H0DqjZFbLTYeMH8HVHy1+KRIjbilW04+rqygsvvGDuWIRWDPU/PnXB0b1Ep1IUhfCTV/hs7QmOxN6ZvfnlbrV4pm0xZ28ubW1eVGeFjt0H57ZAzS5aR1Q0UgCdV0gnNfm5sA1ajSyba1rbcHdz8A6FoSvVWqC1b8LVE7CwPzR9Gnq+D64FLEwshAUodtXy0aNHiY6OJjMz02T/gw8+WOKgRBkzU/3PnvPXmbbmBLvPXwfAzdGO5zvVYGTHGrg7WfDQX9cq0OwZiPwWts2wrgRICqDzF9JBvTfUAZV2ImKtw93NQaeDxo9D7Qfg38mwdwFE/Qwn/oFe76vJUHlPBIVVKtZM0A8//DCHDh1Cp9MZV303rMKt11tpIWlFZkiAijkC7EhsIp+tPcGmE1cAdfbmYe2CealrLSq7Wkk9QPtXYM98tZ4hNsp6WlOkADp/VVuCraNaqHvtNFSpXXrXKg/D3c3B2UtdW6/pU+oCqwlH4I+X4cJ2eGiO1tEJkUeRa4DGjRtHjRo1uHz5Mi4uLhw5coQtW7bQsmVLwsPDSyFEUaoU5U4RbRELoM9dTWXM4v30n7WVTSeuYGujY0jrIDa/1pW3+odZT/ID6hDlho+o29tmahpKkUgBdP7snXLVAZVyTUp5Ge5uLkGt4cXNaheYvas63YQQFqjILUA7duxg48aN+Pj4YGNjg42NDR07dmTq1KmMHTuW/fv3l0acorTcOAdpN8DWAfwaFuolcYlpzNpwil/3XEKfo7YADmwSyMSedahRxbU0oy1dHcbBoWVwdCVcfwcq19Q6ovuTAuiChXSEC1vVBKjlc6VzjV3f5Bru/nX5GO5uDrb20GGs2rXsosEUF0IUQpETIL1ej5ubOktvlSpViI2NpW7dugQHB3PihAVMPS+KxlAA7d/ovsNXr6dmMmfTaX7ceYHM7BwAutfz5dVedWgQWIYTGJYW/0ZQqyecXq8WRQ/4QuuI7s9YAC31P3mEdIDNqIXQpVEHdHItrHlD3e7xLjR42LznLw8k+REWrMgJUMOGDTl48CA1a9akTZs2TJs2DQcHB+bNm0fNmlbwF7MwZUiA7lEAnZKRzXcRZ/ku4hwpGepiiK1DKvNan7q0Ciln/8F1HK8mQPt/hq5vlsmyIMWWlZarALqppqFYpGqt1JbN5Dh1oj7vUPOdu6IMdxeiHCtyAvT222+TmpoKwAcffMCAAQPo1KkT3t7eLF261OwBilJ2n/qfm7cy6T9rKzE30wBoEOjBa73r0qWOj7HwvVwJ7qB+cV6KVNcIe+BdrSMqmBRA35u9s/qzvLBNHRJvrgSoIg53F6IcKnIC1Lt3b+N2zZo1OXr0KNevX8fLy6t8fiGWZ/psdcQTFDgC7LuIc8TcTMPfw4m3B9SnX0MLmL25NOl00GE8LH0aIr9X/7J38tA6qvwZZu+WAuiCBXe4nQBtgxbDS36+ijzcXYhypkijwLKzs7Gzs+Pw4cMm+ytXrizJjzW6chyy08DBHbzzDhO+nprJgm3nAHjvwQYMaGwhszeXtrr9oEodyEiEvQu1jqZghgJoqf8pmDnXBZPh7kKUK0VKgOzs7AgODpa5fsoL4wSITdUVn+8yb8tZUjP1NAj0oHcDv7KNTUs2NuqIMIAdX0F2hrbxFMRQAC0jwApWrRXY2KstNjfOlexcMtxdiHKlyPMAvf3227z55ptcv369NOIRZeke9T9XUzL4Yft5ACY8UKfitfA1egLcAyElHg5aYG1bVhokHFW3pQC6YA4uUK2lul2S+YBkuLsQ5U6RE6BZs2YRERFBYGAgdevWpXnz5iY3YUXuMQP0N5vPkJalp3E1T3rUt+CRUKXFzgHajVa3t81Suz8syeUjoOilALowjN1g24r3ehnuLkS5VOQi6IceeqgUwhBlLvMWXL7dgnBXC1BCUjo/7rgAwISeFbD1x6DFcNjyKVw7BcdXQ5gFrXMnBdCFF9wB+LR464LJcHchyq0iJ0DvvmvBw4JF4cUfUlsQ3PzytCDM3XyGjOwcmlWvRNc6PhoFaAEc3aHVfyDiM3WR1PoDLSfZkALowgtqrdYBJV2CG+ehco3CvU6GuwtRrhW5C0yUE7lXgM/1n3p8Yjo/74oGYGJFbv0xaDNKLXqN2avOJWMppAC68Bxc73TzXihkN5gMdxei3CtyAmRjY4OtrW2BN2ElCiiAnhN+mszsHFqFeNGxVhUNArMwbj7qekYAW2doGoqRFEAXXe7h8Pcjw92FqBCK3AW2YsUKk8dZWVns37+fH374gcmTJ5stMFHKjAXQd7pQYm6msWT3RaCC1/7crd0rsGc+nNmg1oQENNY2HimALrqQjhDxeeEKoWW4uxAVQpEToEGDBuXZ99hjj9GgQQOWLl3KyJEjzRKYKEW3rqtrI4HJGmBfbTpNpj6HtjUr0z5UWn+MKteABo/A4d/UWqDH5msbj7EAupnUpBRWUBuwsYPEaLhxAbyC8z9OhrsLUWGYrQaoTZs2/Pvvv+Y6nShNhi9QrxrG1ZovXr/Fr5G3W38eqKNVZJbLMDHikRVwvYQT6pWUsQC6qZZRWBcH1zvJfkHdYDLcXYgKxSwJUFpaGl9++SXVqlUzx+lEacun/mf2xtNk5yh0rFWFNjW9NQrMggU0htAe6nDoHbO1jcWwfpsUQBdNSAf1Pr9CaBnuLkSFU+QusLsXPVUUheTkZFxcXFi0aJFZgxOlJMaQAKl/EV+4lspv+y4BMKFn3jXBxG0dJ6h1QPsXQZc31ALpspaVBgnH1G1pASqakI6w9Yu8o/mSYuGXJ2S4uxAVTJEToC+++MIkAbKxscHHx4c2bdrg5eVl1uBEKVCUXAXQagvQrA2n0ecodKnjQ4vgyhoGZ+FCOqqfWcxetU6kxztlH4OhANrVRwqgiyqoDehs4Wa0eqtUPddw9zgZ7i5EBVPkBGj48OGlEIYoM0mxkHJZ/SLwb8zZKyms2K+2/kzsKbU/96TTqa1AS5+ByG+h43h1ssSyZKjfCmgqrRRF5eiuFo7H7FFHgzWuKsPdhajAilwDtGDBApYtW5Zn/7Jly/jhhx/MEpQoRYbWH98wcHBh1oZT5CjwQH1fmgRV0jQ0q1C3P3jXhvRE2Luw7K8vBdAlk3s+IBnuLkSFVuQE6OOPP6ZKlbxDpH19ffnoo4/MEpQoRbF36n9OJyTzx4FYAMbLyK/CsbGBDmPV7R1fQXZG2V5fCqBLxpAAHVomw92FqOCKnABduHCBGjXyrqUTHBxMdHS0WYISpSjXCvAz/j2FokDvBn40rOqpbVzWpPFgcA9Q60YO/lp215UC6JIz1AHpbyeuMtxdiAqryAmQr68vBw8ezLP/wIEDeHvL8GmLlpNjbEE451iP1YfiAGn9KTI7R2g7Wt3eNlP9XMuCFECXnJMHVGupbstwdyEqtCInQE8++SRjx45l06ZN6PV69Ho9GzduZNy4cTz55JOlEaMwl2unISMJ7Jz5bL8NigL9GwVQP8BD68isT4vh4OgJ107BidVlc00pgDaPh+bCoDky3F2ICq7Io8A++OADLly4QI8ePbCzU1+ek5PDs88+KzVAlu52/U+qdwNWH7mCTgfjHpB5f4rFyQNaP6+uL7V1BtQbUPpfpob6H+n+KhnvUPUmhKjQipwAOTg4sHTpUj744AOioqJwdnamUaNGBAcXsLaOsBy363+2p6k/q4GNA6njV8bDuMuTNqNg+2x1WPWFbXcKbEuLYQSYFEALIUSJFTkBMqhduza1a0vrgVW5PQP0n1cDsNHB2B7y8ysRN19o9gzs+V6dYbg0EyCTAuhmpXcdIYSoIIpcA/TYY4/x8ccf59n/6aef8vjjj5slKFEKsjPVCd+AA0pNHmpalVq+bhoHVQ60HwM6Gzj9L8QfKr3rmBRAB5bedYQQooIocgK0efNm+vfvn2d/nz592LJli1mCEqUg4QjoM7mpuHJJ588Yaf0xj8o1IOwhdXvbzNK7jhRACyGEWRU5AUpJScHBwSHPfnt7e5KSkswSlCgFt+t/DubU5JFm1ahRxVXjgMqRjuPV+8PL4cb50rmGFEALIYRZFTkBatiwIUuXLs2zf8mSJYSFhZklKGF+V0/sAOAgtRjTXVp/zCqgCYR2ByVHLYouDcYlMKT+RwghzKHIRdDvvPMOjz76KGfOnKF79+4AbNiwgV9++YXffvvN7AEK80g7HwmAe2gbqnu7aBxNOdRxApzZCPt/gi6vg5uP+c6duwBaRoAJIYRZFLkF6MEHH2TlypWcPn2a0aNH8+qrrxITE8PGjRsJCQkphRBFSe0+cYGqWeoyJT0f6KtxNOVUSCcIbA7Z6bD7G/OeWwqghRDC7IqcAAH079+fbdu2kZqayunTp3nkkUcYP348LVq0MHd8ooQURWH12jXY6BRu2vsSWC1E65DKJ53uTi3Q7m8hI9l855YCaCGEMLtiJUAAGzdu5JlnniEwMJDZs2fTr18/9uzZY87YhBnsOHMNx8vqF6hjsKx4XarqDQDvWpB+E/b+YL7zGgugpf5HCCHMpUgJ0KVLl/jggw+oWbMmQ4YMwcvLi6ysLJYvX84HH3xAs2byH7QlURSF6etP0tjmLADOIZIAlSobW2g/Vt3e8ZU695I5GAugm5rnfEIIIQqfAPXr14+wsDCOHj3Kl19+SWxsLF9++WVpxiZKKOLUVfZcuEFTmzPqjsDm2gZUETR5Etz8ITkWDv1a8vNJAbQQQpSKQidA69at4/nnn2fy5Mn0798fW1tbswQwZ84catSogZOTEy1atCAiIuKex2dkZPDWW28RHByMo6MjoaGhzJ8/3+SY5cuXExYWhqOjI2FhYaxYscIssVoTQ+uPN4lU010FdNKCUBbsHKHdaHV720zIySnZ+eIPSwG0EEKUgkInQBERESQnJ9OyZUvatGnD7NmzuXLlSokuvnTpUsaPH89bb73F/v376dSpE3379iU6OrrA1zzxxBNs2LCB77//nhMnTrB48WLq1atnfH7Hjh0MHjyYoUOHcuDAAYYOHcoTTzzBrl27ShSrtQk/cYWoizdpaX9O3VGlNjh5ahtURdHiOXD0hKsn4eQ/JTtX7gVQpQBaCCHMRqcoilKUF9y6dYslS5Ywf/58du/ejV6vZ/r06YwYMQJ396KtLN6mTRuaN2/O3Llzjfvq16/PQw89xNSpU/Mcv2bNGp588knOnj1L5cqV8z3n4MGDSUpK4p9/7nzx9OnTBy8vLxYvXlyouJKSkvD09CQxMREPD48ivSdLoCgKD87exqGYRH6quYFOsd9DkyHw8Ndah1Zx/DsZtk6Haq1g5PriJy8rX4aoRdD5v9D9LfPGKIQQ5UxRvr+LPArMxcWFESNGsHXrVg4dOsSrr77Kxx9/jK+vLw8++GChz5OZmcnevXvp1auXyf5evXqxffv2fF+zatUqWrZsybRp06hatSp16tRh0qRJpKWlGY/ZsWNHnnP27t27wHOC2q2WlJRkcrNm/x5L4FBMIi4OtrRxPK/ulPqfstX2JbB1hEuRcKHg3737kgJoIYQoFcUeBg9Qt25dpk2bxqVLlwrdumJw9epV9Ho9fn5+Jvv9/PyIj4/P9zVnz55l69atHD58mBUrVjBjxgx+++03Xn75ZeMx8fHxRTonwNSpU/H09DTegoKCivReLElOjlr7AzC8XTAOl6PUJ6rKHE1lys0Xmj6lbm+bUbxzSAG0EEKUmhIlQAa2trY89NBDrFq1qsiv1d3VNaAoSp59Bjk5Oeh0On7++Wdat25Nv379mD59OgsXLjRpBSrKOQHefPNNEhMTjbeLFy8W+X1YinVH4zkWl4Sbox0vNraDW9fAxh78G2odWsXTfgzobODUOrWYuaikAFoIIUqNWRKg4qhSpQq2trZ5WmYSEhLytOAYBAQEULVqVTw97xTz1q9fH0VRuHTpEgD+/v5FOieAo6MjHh4eJjdrlJOj8MX6UwCM6BCC542D6hP+DdXRSaJseYdC2CB1e9vMor8+9wKoUgAthBBmpVkC5ODgQIsWLVi/fr3J/vXr19O+fft8X9OhQwdiY2NJSUkx7jt58iQ2NjZUq1YNgHbt2uU557p16wo8Z3ny9+E4TlxOxt3JjpEda0LMPvUJ6f7STofx6v3h5XDjQtFea5gBWrq/hBDC7DRLgAAmTpzId999x/z58zl27BgTJkwgOjqaUaNGAWrX1LPPPms8/qmnnsLb25vnnnuOo0ePsmXLFl577TVGjBiBs7MzAOPGjWPdunV88sknHD9+nE8++YR///2X8ePHa/EWy4w+R2HGv2rrz/Mda+LpYn8nAZICaO0ENoWa3dSurB2zi/ZaKYAWQohSo2kCNHjwYGbMmMGUKVNo2rQpW7Zs4e+//yY4OBiAuLg4kzmB3NzcWL9+PTdv3qRly5Y8/fTTDBw4kFmzZhmPad++PUuWLGHBggU0btyYhQsXsnTpUtq0aVPm768s/XUwltMJKXg62/NcxxDQZ9/5ApUWIG0ZFknd9xOkXi3ca6QAWgghSlWR5wGqCKxtHqBsfQ69vtjC2aupvNa7Li93qwWXj8Dc9uDgBm9Eq+tUCW0oCnzbTV3VvbDz+VyMhO8fAFdfmHRSaoCEEKIQSnUeIGF5/oiK5ezVVLxc7BnWPkTdaez+aibJj9Z0uju1QLvnQUbKPQ8HTLu/JPkRQgizkwTIymXpc5i1Ua39ebFLKG6OduoTMXvV+8BmGkUmTNQfCJVDIf0m7Pvx/sdLAbQQQpQqSYCs3Ip9MVy4dgtvVweebRd854lYGQFmUWxsocNYdXvHbMjOvPfxsfvVeymAFkKIUiEJkBXLzL7T+vNS11BcHG63/mSlqzVAAFVlBJjFaPwkuPlBUgwc/q3g47LS4MpxdVtagIQQolRIAmTFftt7iUs30vBxd+TpNrlaf+IPQU62OoOwp/Uu61Hu2Dupa4QBbJ0BOTn5H2ecAdpXZoAWQohSIgmQlcrI1jP7duvP6K6hODvkKnQ21P9UbSEFtJam5Qhw9ICrJ+DkmvyPkQJoIYQodZIAWalfIy8Sm5iOn4cjQ1pXN33SWAAt3V8Wx8lTTYIAtn6hDpG/mxRACyFEqZMEyAqlZ+mZvek0AK90q4WT/V3D3KUA2rK1fQlsHeHSbojekfd5KYAWQohSJwmQFVq8O5rLSRkEejrxRKu7anzSbsI1NTmSIfAWyt0fmg5Rt7fOMH0udwG0/PyEEKLUSAJkZdIy9cwJPwPAK91r42h3d+vP7dYDrxBw9S7b4EThtR8L6ODU2jsj9sC0ANo9QLPwhBCivJMEyMr8vOsCV5IzqOblzGMtquU9QOp/rIN3KIQNUre3zbyzXwqghRCiTEgCZEVuZWYz93brz9jutXGwy+fHZ2gBkvofy2dYJPXQb3Dz9qK/hp+fFEALIUSpkgTIivy44wLXUjMJ9nbh4eZV8z/IOAReWoAsXmAzqNFF7fLaPlvdZxgBJvU/QghRqiQBshIpGdl8s/lO64+9bT4/uqRYSI4DnQ0ENCnjCEWxdJyg3u/7ERIv5SqAbqpZSEIIURFIAmQlfth+nhu3sqhZxZVBTQuYHdiwArxPfXBwLbvgRPHV7Komq9lpsGqsFEALIUQZkQTICiSlZzFvy1kAxj1QG7v8Wn8g1/w/0v1lNXS6O61AZzao91IALYQQpU4SICuwYOt5EtOyqOXrxoDG91gbKvcSGMJ61H8QKte881gKoIUQotRJAmThEm9l8d1WtfVn/AO1sbUpoGUgJwdiDCPApAXIqtjYQvsxdx5LAbQQQpQ6SYAs3Pdbz5Kcnk1dP3f6NbxHXcj1s5CRCHZO4BtWdgEK82jyFFSqDg5uENRa62iEEKLcs9M6AFGwG6mZzN92HoAJPWtjU1DrD9zp/vJvDLb2pR+cMC97J/jPJshOB9cqWkcjhBDlniRAFuzbiLOkZGQTFuBBrzD/ex8sC6BaP0l8hBCizEgXmIW6lpLBwu3nAZjQs869W39AJkAUQgghikASIAs1b8tZbmXqaVTVkwfq+977YH0WxB1Ut6UFSAghhLgvSYAs0JXkDH7YcR6AiT3roLvfnDCXj4A+A5w8TYdTCyGEECJfkgBZoK83nyE9K4emQZXoWtfn/i8w1P8ENpcJ9IQQQohCkATIwlxOSmfRzgtAIVt/QOp/hBBCiCKSBMjCzA0/Q0Z2Di2DvehUu5CjgowTIEr9jxBCCFEYkgBZkLjENH7ZFQ0UofUnIwWuHFO3JQESQgghCkUSIAvy1abTZOpzaFOjMu1CvQv3orgDoOSAeyC432euICGEEEIAkgBZjEs3brE08iKgzvtTqNYfkBXghRBCiGKQBMhCfLXpNFl6hQ61vGlbs5CtPyAF0EIIIUQxSAJkAaKv3WLZnksATHigTtFeHCNLYAghhBBFJQmQBfhy4ymycxQ61/GhZUjlwr8w9SrcVIfME9C0VGITQgghyiNJgDR27moqv++PAWDCA7WL9uLY28PfvWuDcyXzBiaEEEKUY5IAaezLDafQ5yh0r+dLs+peRXux1P8IIYQQxSIJkIZOJ6SwMsrQ+lPE2h+Q+h8hhBCimCQB0tCsDafIUaBnmB+NqnkW7cWKcqcFKFBagIQQQoiikARIIycvJ/PnwVgAxhe19gcg8SLcugo2duDfyMzRCSGEEOWbJEAamfnvKRQF+jb0p0FgEVt/4E7rj19DsHcyb3BCCCFEOScJkAaOxSWx+lAcOh2ML07tD+Sq/5HuLyGEEKKoJAHSwBfrTwLQv1EAdf3di3cSKYAWQgghik0SoDJ26FIi645evt36U4zaH4Ac/Z05gKQAWgghhCgySYDK2Ix/1dafQU0CqeVbzNafqychKxXsXcGnrhmjE0IIISoGSYDKUNTFm2w4noCNDsb2KGbrD+Qa/t4UbGzNEpsQQghRkdhpHUBF0ySoErV83Kjp41b8k0gBtBBCCFEikgCVoaZBlVg5uj1pWfqSnUgmQBRCCCFKRLrAyphOp8PFoQR5Z1Y6XD6ibssIMCGEEKJYJAGyNpcPQ04WuHhDpepaRyOEEEJYJUmArE3u+X90Om1jEUIIIayUJEDWxlD/I91fQgghRLFJAmRtYm+3AEkBtBBCCFFskgBZk/REdRJEkCHwQgghRAlIAmRNDMtfVKoOrlW0jUUIIYSwYpIAWRNZAFUIIYQwC0mArIlMgCiEEEKYhSRA1sTQBSYtQEIIIUSJSAJkLZLjISkGdDYQ0ETraIQQQgirJgmQtTDU//jUA8cSLKQqhBBCCEmArIbU/wghhBBmIwmQtTBMgCjz/wghhBAlJgmQNVAUGQIvhBBCmJEkQNbg+llIvwm2juDXQOtohBBCCKsnCZA1MLT+BDQGW3ttYxFCCCHKAUmArIEUQAshhBBmJQmQNYiV+h8hhBDCnDRPgObMmUONGjVwcnKiRYsWREREFHhseHg4Op0uz+348ePGYxYuXJjvMenp6WXxdsxPnwVxB9RtGQEmhBBCmIWdlhdfunQp48ePZ86cOXTo0IFvvvmGvn37cvToUapXr17g606cOIGHh4fxsY+Pj8nzHh4enDhxwmSfk5OTeYMvKwnHIDsdHD2hcqjW0QghhBDlgqYJ0PTp0xk5ciTPP/88ADNmzGDt2rXMnTuXqVOnFvg6X19fKlWqVODzOp0Of39/c4erDWP9T1Ow0bzBTgghhCgXNPtGzczMZO/evfTq1ctkf69evdi+ffs9X9usWTMCAgLo0aMHmzZtyvN8SkoKwcHBVKtWjQEDBrB///57ni8jI4OkpCSTm8WQ+h8hhBDC7DRLgK5evYper8fPz89kv5+fH/Hx8fm+JiAggHnz5rF8+XJ+//136tatS48ePdiyZYvxmHr16rFw4UJWrVrF4sWLcXJyokOHDpw6darAWKZOnYqnp6fxFhQUZJ43aQ4xMgO0EEIIYW46RVEULS4cGxtL1apV2b59O+3atTPu//DDD/npp59MCpvvZeDAgeh0OlatWpXv8zk5OTRv3pzOnTsza9asfI/JyMggIyPD+DgpKYmgoCASExNNao3KXGYqTA0CRQ8Tj4FHoHaxCCGEEBYuKSkJT0/PQn1/a9YCVKVKFWxtbfO09iQkJORpFbqXtm3b3rN1x8bGhlatWt3zGEdHRzw8PExuFiHuoJr8uAdI8iOEEEKYkWYJkIODAy1atGD9+vUm+9evX0/79u0LfZ79+/cTEBBQ4POKohAVFXXPYyyWof5HJkAUQgghzErTUWATJ05k6NChtGzZknbt2jFv3jyio6MZNWoUAG+++SYxMTH8+OOPgDpKLCQkhAYNGpCZmcmiRYtYvnw5y5cvN55z8uTJtG3bltq1a5OUlMSsWbOIioriq6++0uQ9lohhBJjU/wghhBBmpWkCNHjwYK5du8aUKVOIi4ujYcOG/P333wQHBwMQFxdHdHS08fjMzEwmTZpETEwMzs7ONGjQgNWrV9OvXz/jMTdv3uSFF14gPj4eT09PmjVrxpYtW2jdunWZv78SkwRICCGEKBWaFUFbsqIUUZWaW9dhWg11+/Xz4OylTRxCCCGElbCKImhxH4bh75VDJfkRQgghzEwSIEslEyAKIYQQpUYSIEsl9T9CCCFEqZEEyBIpSq4ZoKUFSAghhDA3SYAsUeIlSE0AGzvwb6R1NEIIIUS5IwmQJTLU//iGgb2ztrEIIYQQ5ZAkQJbIWP8j3V9CCCFEaZAEyBLJCvBCCCFEqZIEyNLk5EBslLotLUBCCCFEqZAEyNJcOwWZyWDvAlXqah2NEEIIUS5JAmRpDPU/AU3BVtOl2oQQQohySxIgSyMTIAohhBClThIgSyMF0EIIIUSpkwTIkmRnQPwhdTtQEiAhhBCitEgCZEkuH4acLHCuDF4hWkcjhBBClFuSAFmS3N1fOp22sQghhBDlmAwzsiSyAKoQZqPX68nKytI6DCGEmTk4OGBjU/L2G0mALIksgSFEiSmKQnx8PDdv3tQ6FCFEKbCxsaFGjRo4ODiU6DySAFmK9CS4elLdlgJoIYrNkPz4+vri4uKCTrqThSg3cnJyiI2NJS4ujurVq5fo37ckQJYiLgpQwLM6uPloHY0QVkmv1xuTH29vb63DEUKUAh8fH2JjY8nOzsbe3r7Y55EiaEthrP9ppm0cQlgxQ82Pi4uLxpEIIUqLoetLr9eX6DySAFkKqf8Rwmyk20uI8stc/74lAbIUhhYgqf8RQhRSeHg4Op3uvgXfISEhzJgxo0xisnTDhw/noYce0jqMMqPT6Vi5ciUA58+fR6fTERUVVarX7Nq1K+PHjy/Va5iDJECWIPkyJF0CdBDYVOtohBBl7Ouvv8bd3Z3s7GzjvpSUFOzt7enUqZPJsREREeh0Ok6ePEn79u2Ji4vD09MTgIULF1KpUqWyDN1EYROtkJAQdDodOp0OW1tbAgMDGTlyJDdu3CjS9crqi3bhwoXodDr69Oljsv/mzZvodDrCw8NLPQZzCAoKIi4ujoYNG5rlfAUl4L///jvvv/++Wa5RmiQBsgSxt1t/fOqCo7u2sQghyly3bt1ISUlhz549xn0RERH4+/sTGRnJrVu3jPvDw8MJDAykTp06ODg44O/vb5VdflOmTCEuLo7o6Gh+/vlntmzZwtixY7UOq0B2dnZs2LCBTZs2mfW8mZmZZj3fvdja2uLv74+dXemOf6pcuTLu7pb/XSYJkCWQCRCFqNDq1q1LYGCgSUtCeHg4gwYNIjQ0lO3bt5vs79atm3Hb8Bd4eHg4zz33HImJicbWlffee8/4ulu3bjFixAjc3d2pXr068+bNM4nh0KFDdO/eHWdnZ7y9vXnhhRdISUkxPp9fa8tDDz3E8OHDjc9fuHCBCRMmGK9/L+7u7vj7+1O1alW6devGs88+y759+4zPX7t2jSFDhlCtWjVcXFxo1KgRixcvNj4/fPhwNm/ezMyZM43XO3/+PABHjhyhf//+eHh44O7uTqdOnThz5ozJ9T/77DMCAgLw9vbm5Zdfvu+kma6urjz33HO88cYb9zzufp+joQtu6tSpxkTW0DX166+/0qlTJ5ydnWnVqhUnT54kMjKSli1b4ubmRp8+fbhy5YrxXJGRkfTs2ZMqVarg6elJly5dTD7Du93dBTZ8+HDjZ5f7Zvg9XLRoES1btjT+rJ566ikSEhKM5zL8Hnp5eaHT6Ux+F3L/rty4cYNnn30WLy8vXFxc6Nu3L6dOnTI+b2i5XLt2LfXr1ze+17i4uHt+1iUlCZAlMBRAB8oIMCHMTVEUbmVma3JTFKXQcXbt2tWkdWHTpk107dqVLl26GPdnZmayY8cO4xdPbu3bt2fGjBl4eHgQFxdHXFwckyZNMj7/+eef07JlS/bv38/o0aN56aWXOH78OKAmR3369MHLy4vIyEiWLVvGv//+yyuvvFLo+H///XeqVatmbNkpypdXTEwMf/31F23atDHuS09Pp0WLFvz1118cPnyYF154gaFDh7Jr1y4AZs6cSbt27fjPf/5jvF5QUBAxMTF07twZJycnNm7cyN69exkxYoRJ9+KmTZs4c+YMmzZt4ocffmDhwoUsXLjwvnG+9957HDp0iN9++y3f5wv7OW7YsIFjx46xfv16/vrrL+P+d999l7fffpt9+/ZhZ2fHkCFD+O9//8vMmTOJiIjgzJkz/O9//zMen5yczLBhw4iIiGDnzp3Url2bfv36kZycXKjPfebMmcbPLi4ujnHjxuHr60u9evUA9fft/fff58CBA6xcuZJz584Zk5ygoCCWL18OwIkTJ4iLi2PmzJn5Xmf48OHs2bOHVatWsWPHDhRFoV+/fiZJ561bt/jss8/46aef2LJlC9HR0Sa/v6VB5gHSmqLc6QKTFiAhzC4tS0/Y/9Zqcu2jU3rj4lC4/2a7du3KhAkTyM7OJi0tjf3799O5c2f0ej2zZs0CYOfOnaSlpeWbADk4OODp6YlOp8Pf3z/P8/369WP06NEAvP7663zxxReEh4dTr149fv75Z9LS0vjxxx9xdXUFYPbs2QwcOJBPPvkEPz+/+8ZfuXJlbG1tja0F9/P666/z9ttvo9frSU9Pp02bNkyfPt34fNWqVU2+AMeMGcOaNWtYtmwZbdq0wdPTEwcHB1xcXEyu99VXX+Hp6cmSJUuMc8TUqVPH5NpeXl7Mnj0bW1tb6tWrR//+/dmwYQP/+c9/7hlzYGAg48aN46233sq3kLqwn6OrqyvfffedcTi3oeVq0qRJ9O7dG4Bx48YxZMgQNmzYQIcOHQAYOXKkSaLWvXt3k+t/8803eHl5sXnzZgYMGHDP9wLg6elprB/7/fff+frrr/n333+Nn+eIESOMx9asWZNZs2bRunVrUlJScHNzo3LlygD4+voWWHt26tQpVq1axbZt22jfvr3xcwoKCmLlypU8/vjjgDqFxddff01oaCgAr7zyClOmTLnveygJaQHS2o1zkHYDbB3AzzyFaUII69OtWzdSU1OJjIwkIiKCOnXq4OvrS5cuXYiMjCQ1NZXw8HCqV69OzZo1i3z+xo0bG7cNSZKhO+PYsWM0adLE+KUN0KFDB3Jycjhx4kTJ31w+XnvtNaKiojh48CAbNmwAoH///sa5XfR6PR9++CGNGzfG29sbNzc31q1bR3R09D3PGxUVRadOne45QV6DBg2wtbU1Pg4ICDB+Fvfz+uuvc+XKFebPn5/nucJ+jo0aNcp3GYfcPyNDstSoUSOTfbnjTEhIYNSoUdSpU8eYzKSkpNz3M7rb/v37efbZZ/nqq6/o2LGjyf5BgwYRHByMu7s7Xbt2BSjS+Y8dO4adnZ1J6563tzd169bl2LFjxn0uLi7G5AeK9jMpLmkB0pqh/se/EdiVbF0TIURezva2HJ3SW7NrF1atWrWoVq0amzZt4saNG3Tp0gUAf39/atSowbZt29i0aVOev/oL6+6EQKfTkZOTA6jdhAXV7Bj229jY5OnSK8lis1WqVKFWrVoA1K5dmxkzZtCuXTs2bdrEAw88wOeff84XX3zBjBkzaNSoEa6urowfP/6+RcPOzs73vfa9Pov7qVSpEm+++SaTJ0/O08pSmM8RMEmQCorLcPzd+3LHOXz4cK5cucKMGTMIDg7G0dGRdu3aFamwOj4+ngcffJCRI0cycuRI4/7U1FR69epFr169WLRoET4+PkRHR9O7d+8inb+gbuC7P6v8fiZF6UIuDmkB0poUQAtRqnQ6HS4Odprcijo6q1u3boSHhxMeHm78axugS5curF27lp07d+bb/WXg4OBQrNlxw8LCiIqKIjU11bhv27Zt2NjYGLuPfHx8TOp69Ho9hw8fNsv1AWOLTFpaGqCOghs0aBDPPPMMTZo0oWbNmiaFswVdr3HjxkRERJQoObufMWPGYGNjk6fmpTCfozlFREQwduxY+vXrR4MGDXB0dOTq1auFfn16ejqDBg2iXr16Jt2PAMePH+fq1at8/PHHdOrUiXr16uVpkSnMjMxhYWFkZ2cba7dALXA/efIk9evXL3SspUESIK3FygSIQghVt27d2Lp1K1FRUcYWIFAToG+//Zb09PR7JkAhISGkpKSwYcMGrl69ajJ8/l6efvppnJycGDZsGIcPH2bTpk2MGTOGoUOHGrtiunfvzurVq1m9ejXHjx9n9OjReeZ/CQkJYcuWLcTExNz3izg5OZn4+Hji4uLYvXs3r732GlWqVDHWidSqVYv169ezfft2jh07xosvvkh8fHye6+3atYvz589z9epVcnJyeOWVV0hKSuLJJ59kz549nDp1ip9++smsXXlOTk5MnjzZWJtlUJjP0Zxq1arFTz/9xLFjx9i1axdPP/10oVrADF588UUuXrzIrFmzuHLlCvHx8cTHx5OZmUn16tVxcHDgyy+/5OzZs6xatSrP3D7BwcHodDr++usvrly5YjLazaB27doMGjSI//znP2zdupUDBw7wzDPPULVqVQYNGlTiz6AkJAHSkj4bYqPUbWkBEqLC69atG2lpadSqVcvkC7NLly4kJycTGhpKUFBQga9v3749o0aNYvDgwfj4+DBt2rRCXdfFxYW1a9dy/fp1WrVqxWOPPUaPHj2YPXu28ZgRI0YwbNgwnn32Wbp06UKNGjXyJGNTpkzh/PnzhIaG4uNz70Wd//e//xEQEEBgYCADBgzA1dWV9evXGxexfeedd2jevDm9e/ema9eu+Pv75yk8njRpEra2toSFhRm7aLy9vdm4cSMpKSl06dKFFi1a8O2335Zo0cz8DBs2LE8tVmE+R3OaP38+N27coFmzZgwdOpSxY8fi6+tb6Ndv3ryZuLg4wsLCCAgIMN62b9+Oj48PCxcuZNmyZYSFhfHxxx/z2Wefmby+atWqTJ48mTfeeAM/P78CRw0uWLCAFi1aMGDAANq1a4eiKPz9999m/5kUlU4p7U42K5SUlISnpyeJiYl4eHiU3oXiD8HXHcHRA16/ADaSjwpREunp6Zw7d44aNWrg5OSkdThCiFJwr3/nRfn+lm9cLRnX/2oqyY8QQghRhuRbV0vGCRCl/kcIIYQoS5IAaUkmQBRCCCE0IQmQVjJvweWj6nZVaQESQgghypIkQFqJPwSKHtz8wKOq1tEIIYQQFYokQFox1P9UbQFFnCxNCCGEECUjCZBWZAJEIYQQQjOSAGnF2AIkCZAQQghR1iQB0sKt63D9rLod2EzbWIQQQogKSBIgLcTuV+8r1wSXytrGIoSwWuHh4eh0ujxrct0tJCSEGTNmlElM5rBw4UIqVaqkdRhlpmvXrowfP974uCx+Xu+99x5NmzYt1WtYOkmAtCD1P0KIXL7++mvc3d3Jzs427ktJScHe3p5OnTqZHBsREYFOp+PkyZO0b9+euLg4PD09AfMmDsOHD0en0xlv3t7e9OnTh4MHDxbpPGX1RXv+/Hl0Oh2+vr4kJyebPNe0aVPee++9Uo/BXCIjI3nhhRfMdj6dTsfKlStN9k2aNIkNGzaY7RrWSBIgLcTIBIhCiDu6detGSkoKe/bsMe6LiIjA39+fyMhIk1Xdw8PDCQwMpE6dOjg4OODv74+ulEaS9unTh7i4OOLi4tiwYQN2dnYMGDCgVK5lLsnJyXkW7SwpvV5PTk6OWc95Lz4+Pri4uJTqNdzc3IwLz1ZUkgCVNUWRAmghhIm6desSGBhIeHi4cV94eDiDBg0iNDSU7du3m+w3rMKeuwssPDyc5557jsTERGOrTe5Wj1u3bjFixAjc3d2pXr068+bNu29cjo6O+Pv74+/vT9OmTXn99de5ePEiV65cMR7z+uuvU6dOHVxcXKhZsybvvPMOWVlZgNoiNXnyZA4cOGCMaeHChQDcvHmTF154AT8/P5ycnGjYsCF//fWXyfXXrl1L/fr1cXNzMyZj9zNmzBimT59OQkJCgcfcuHGDZ599Fi8vL1xcXOjbty+nTp0yPm9oSfvrr78ICwvD0dGRCxcuEBISwgcffMCzzz6Lm5sbwcHB/PHHH1y5coVBgwbh5uZGo0aNTBLZa9euMWTIEKpVq4aLiwuNGjVi8eLF93wPubvAFi5caNISd/fPNjIykp49e1KlShU8PT3p0qUL+/btMzkXwMMPP4xOpzM+vrtlLicnhylTplCtWjUcHR1p2rQpa9asMT5vaGH7/fff6datGy4uLjRp0oQdO3bc871YMkmAylpSLKRcBp0t+DfWOhohyj9FgcxUbW6KUugwu3btyqZNm4yPN23aRNeuXenSpYtxf2ZmJjt27DAmQLm1b9+eGTNm4OHhYWy1mTRpkvH5zz//nJYtW7J//35Gjx7NSy+9xPHjxwsdX0pKCj///DO1atUyaTlwd3dn4cKFHD16lJkzZ/Ltt9/yxRdfADB48GBeffVVGjRoYIxp8ODB5OTk0LdvX7Zv386iRYs4evQoH3/8Mba2tsbz3rp1i88++4yffvqJLVu2EB0dbfJ+CjJkyBBq1arFlClTCjxm+PDh7Nmzh1WrVrFjxw4URaFfv37GxM1w/alTp/Ldd99x5MgRfH19Afjiiy/o0KED+/fvp3///gwdOpRnn32WZ555hn379lGrVi2effZZlNs/+/T0dFq0aMFff/3F4cOHeeGFFxg6dCi7du0q1Oc+ePBg42cXFxfH4sWLsbOzo0OHDoDa4jVs2DAiIiLYuXMntWvXpl+/fsZuwMjISAAWLFhAXFyc8fHdZs6cyeeff85nn33GwYMH6d27Nw8++KBJYgjw1ltvMWnSJKKioqhTpw5Dhgwx6bq1JnZaB1DhGFp/fMPAoXSbOIUQQNYt+ChQm2v/Xyw4uBbq0K5duzJhwgSys7NJS0tj//79dO7cGb1ez6xZswDYuXMnaWlp+SZADg4OeHp6otPp8Pf3z/N8v379GD16NKC22nzxxReEh4dTr169AmP666+/cHNzAyA1NZWAgAD++usvbGzu/O389ttvG7dDQkJ49dVXWbp0Kf/9739xdnbGzc0NOzs7k5jWrVvH7t27OXbsGHXq1AGgZs2aJtfOysri66+/JjQ0FIBXXnnlnkmNgU6n4+OPP2bgwIFMmDDB+HqDU6dOsWrVKrZt20b79u0B+PnnnwkKCmLlypU8/vjjxuvPmTOHJk2a5PkcX3zxRQD+97//MXfuXFq1amV83euvv067du24fPky/v7+VK1a1SRxGzNmDGvWrGHZsmW0adPmvu/H2dkZZ2dnAM6cOcMrr7zCRx99RM+ePQHo3r27yfHffPMNXl5ebN68mQEDBuDj4wNApUqV8v29MPjss894/fXXefLJJwH45JNP2LRpEzNmzOCrr74yHjdp0iT69+8PwOTJk2nQoAGnT5++5++RpZIWoLJmXABVur+EEHd069aN1NRUIiMjiYiIoE6dOvj6+tKlSxciIyNJTU0lPDyc6tWr50kWCqNx4zstzoYk6V7dRIaYoqKiiIqKYteuXfTq1Yu+ffty4cIF4zG//fYbHTt2xN/fHzc3N9555x2io6Pved6oqCiqVatmTH7y4+LiYpK8BAQE3Ddeg969e9OxY0feeeedPM8dO3YMOzs7k+TD29ubunXrcuzYMeM+BwcHk8/MIPc+Pz8/ABo1apRnnyFWvV7Phx9+SOPGjfH29sbNzY1169bd9zO6W2JiIgMGDKBv37689tprxv0JCQmMGjWKOnXq4OnpiaenJykpKUU6f1JSErGxscZWJYMOHTqYfCZg+v4DAgJM3qu1kRagsib1P0KULXsXtSVGq2sXUq1atahWrRqbNm3ixo0bdOnSBQB/f39q1KjBtm3b2LRpU56/+Asdir29yWOdTnffwl5XV1dq1aplfNyiRQs8PT359ttv+eCDD9i5cydPPvkkkydPpnfv3nh6erJkyRI+//zze57X0KJR1HiVInQpfvzxx7Rr184kWQAKPIeiKCbF5M7OzvkWl+eOy/B8fvsMn+3nn3/OF198wYwZM2jUqBGurq6MHz+ezMzMQr8XvV7P4MGD8fDw4NtvvzV5bvjw4Vy5coUZM2YQHByMo6Mj7dq1K9L5747d4O7PBO79Xq2NJEBlKScHYqPUbRkBJkTZ0OkK3Q2ltW7duhEeHs6NGzdMvri7dOnC2rVr2blzJ88991yBr3dwcECv15dafDqdDhsbG9LS0gDYtm0bwcHBvPXWW8ZjcrcOFRRT48aNuXTpEidPnrxnK1BJtG7dmkceeYQ33njDZH9YWBjZ2dns2rXL2AV27do1Tp48Sf369c0eR0REBIMGDeKZZ54B1GTh1KlTRbrWhAkTOHToEJGRkTg5OeU5/5w5c+jXrx8AFy9e5OrVqybH2Nvb3/P3wsPDg8DAQLZu3Urnzp2N+7dv307r1q0LHae1kQSoLF07DRlJYOcMPub/hyaEsG7dunXj5ZdfJisry9gCBGoC9NJLL5Genp5v/Y9BSEgIKSkpbNiwgSZNmuDi4lKi4dQZGRnEx8cD6sip2bNnk5KSwsCBAwG11So6OpolS5bQqlUrVq9ezYoVK/LEdO7cOWO3l7u7O126dKFz5848+uijTJ8+nVq1anH8+HF0Oh19+vQpdrx3+/DDD2nQoAF2dne+6mrXrs2gQYP4z3/+wzfffIO7uztvvPEGVatWZdCgQWa7tkGtWrVYvnw527dvx8vLi+nTpxMfH1/oBGjBggXMmTOHFStWYGNjY/x5uLm54ebmRq1atfjpp59o2bIlSUlJvPbaa3la2EJCQtiwYQMdOnTA0dERLy+vPNd57bXXePfddwkNDaVp06YsWLCAqKgofv7555J/CBZKaoDKUnIsuHhDQBOwldxTCGGqW7dupKWlUatWLWMtCagJUHJyMqGhoQQFBRX4+vbt2zNq1CgGDx6Mj48P06ZNK1E8a9asISAggICAANq0aUNkZCTLli2ja9euAAwaNIgJEybwyiuv0LRpU7Zv356n7ubRRx+lT58+dOvWDR8fH+MQ8OXLl9OqVSuGDBlCWFgY//3vf83eelWnTh1GjBhBenq6yf4FCxbQokULBgwYQLt27VAUhb///jtPt5s5vPPOOzRv3pzevXvTtWtX/P39eeihhwr9+s2bN6PX63nwwQeNP4uAgADjXEfz58/nxo0bNGvWjKFDhzJ27FjjiDWDzz//nPXr1xMUFESzZvkvvzR27FheffVVXn31VRo1asSaNWtYtWoVtWvXLvZ7t3Q6pSidqhVEUlISnp6eJCYm4uHhYd6TKwqkJ4JzJfOeVwhBeno6586do0aNGnm6CoQQ5cO9/p0X5ftbWoDKmk4nyY8QQgihMUmAhBBCCFHhSAIkhBBCiApHEiAhhBBCVDiSAAkhhBCiwpEESAhR7sjgViHKL3P9+5YESAhRbhjmcbl165bGkQghSothmQ9bW9sSnUdm4xNClBu2trZUqlTJuDiji4tLvus5CSGsU05ODleuXMHFxcVkhu/ikARICFGu+Pv7A9a7QrUQ4t5sbGyoXr16if+4kQRICFGu6HQ6AgIC8PX1JSsrS+twhBBm5uDggI1NySt4NE+A5syZw6effkpcXBwNGjRgxowZdOrUKd9jw8PD810I8NixY9SrV8/4ePny5bzzzjucOXOG0NBQPvzwQx5++OFSew9CCMtja2tb4hoBIUT5pWkR9NKlSxk/fjxvvfUW+/fvp1OnTvTt25fo6Oh7vu7EiRPExcUZb7kXa9uxYweDBw9m6NChHDhwgKFDh/LEE0+wa9eu0n47QgghhLASmi6G2qZNG5o3b87cuXON++rXr89DDz3E1KlT8xxvaAG6ceMGlSpVyvecgwcPJikpiX/++ce4r0+fPnh5eRlXIb6fUl0MVQghhBClwioWQ83MzGTv3r306tXLZH+vXr3Yvn37PV/brFkzAgIC6NGjB5s2bTJ5bseOHXnO2bt373ueMyMjg6SkJJObEEIIIcovzWqArl69il6vx8/Pz2S/n58f8fHx+b4mICCAefPm0aJFCzIyMvjpp5/o0aMH4eHhdO7cGYD4+PginRNg6tSpTJ48Oc9+SYSEEEII62H43i5M55bmRdB3D2NTFKXAoW1169albt26xsft2rXj4sWLfPbZZ8YEqKjnBHjzzTeZOHGi8XFMTAxhYWEEBQUV6b0IIYQQQnvJycl4enre8xjNEqAqVapga2ubp2UmISEhTwvOvbRt25ZFixYZH/v7+xf5nI6Ojjg6Ohofu7m5cfHiRdzd3c0+iVpSUhJBQUFcvHhR6otKkXzOZUM+57Ihn3PZkc+6bJTW56woCsnJyQQGBt73WM0SIAcHB1q0aMH69etNhqivX7+eQYMGFfo8+/fvJyAgwPi4Xbt2rF+/ngkTJhj3rVu3jvbt2xf6nDY2NlSrVq3QxxeHh4eH/OMqA/I5lw35nMuGfM5lRz7rslEan/P9Wn4MNO0CmzhxIkOHDqVly5a0a9eOefPmER0dzahRowC1ayomJoYff/wRgBkzZhASEkKDBg3IzMxk0aJFLF++nOXLlxvPOW7cODp37swnn3zCoEGD+OOPP/j333/ZunWrJu9RCCGEEJZH0wRo8ODBXLt2jSlTphAXF0fDhg35+++/CQ4OBiAuLs5kTqDMzEwmTZpETEwMzs7ONGjQgNWrV9OvXz/jMe3bt2fJkiW8/fbbvPPOO4SGhrJ06VLatGlT5u9PCCGEEJZJ03mAKqKMjAymTp3Km2++aVJ3JMxLPueyIZ9z2ZDPuezIZ102LOFzlgRICCGEEBWOpkthCCGEEEJoQRIgIYQQQlQ4kgAJIYQQosKRBEgIIYQQFY4kQGVozpw51KhRAycnJ1q0aEFERITWIZU7U6dOpVWrVri7u+Pr68tDDz3EiRMntA6rXJs6dSo6nY7x48drHUq5FBMTwzPPPIO3tzcuLi40bdqUvXv3ah1WuZKdnc3bb79NjRo1cHZ2pmbNmkyZMoWcnBytQ7NqW7ZsYeDAgQQGBqLT6Vi5cqXJ84qi8N577xEYGIizszNdu3blyJEjZRafJEBlZOnSpYwfP5633nqL/fv306lTJ/r27Wsyz5Eouc2bN/Pyyy+zc+dO1q9fT3Z2Nr169SI1NVXr0MqlyMhI5s2bR+PGjbUOpVy6ceMGHTp0wN7enn/++YejR4/y+eefU6lSJa1DK1c++eQTvv76a2bPns2xY8eYNm0an376KV9++aXWoVm11NRUmjRpwuzZs/N9ftq0aUyfPp3Zs2cTGRmJv78/PXv2JDk5uWwCVESZaN26tTJq1CiTffXq1VPeeOMNjSKqGBISEhRA2bx5s9ahlDvJyclK7dq1lfXr1ytdunRRxo0bp3VI5c7rr7+udOzYUeswyr3+/fsrI0aMMNn3yCOPKM8884xGEZU/gLJixQrj45ycHMXf31/5+OOPjfvS09MVT09P5euvvy6TmKQFqAxkZmayd+9eevXqZbK/V69ebN++XaOoKobExEQAKleurHEk5c/LL79M//79eeCBB7QOpdxatWoVLVu25PHHH8fX15dmzZrx7bffah1WudOxY0c2bNjAyZMnAThw4ABbt241WWVAmNe5c+eIj483+V50dHSkS5cuZfa9qOlSGBXF1atX0ev1eVak9/Pzy7NyvTAfRVGYOHEiHTt2pGHDhlqHU64sWbKEvXv3smfPHq1DKdfOnj3L3LlzmThxIv/3f//H7t27GTt2LI6Ojjz77LNah1duvP766yQmJlKvXj1sbW3R6/V8+OGHDBkyROvQyi3Dd19+34sXLlwokxgkASpDOp3O5LGiKHn2CfN55ZVXOHjwoCyEa2YXL15k3LhxrFu3DicnJ63DKddycnJo2bIlH330EQDNmjXjyJEjzJ07VxIgM1q6dCmLFi3il19+oUGDBkRFRTF+/HgCAwMZNmyY1uGVa1p+L0oCVAaqVKmCra1tntaehISEPNmvMI8xY8awatUqtmzZQrVq1bQOp1zZu3cvCQkJtGjRwrhPr9ezZcsWZs+eTUZGBra2thpGWH4EBAQQFhZmsq9+/fosX75co4jKp9dee4033niDJ598EoBGjRpx4cIFpk6dKglQKfH39wfUlqCAgADj/rL8XpQaoDLg4OBAixYtWL9+vcn+9evX0759e42iKp8UReGVV17h999/Z+PGjdSoUUPrkMqdHj16cOjQIaKiooy3li1b8vTTTxMVFSXJjxl16NAhzzQOJ0+eJDg4WKOIyqdbt25hY2P6dWhrayvD4EtRjRo18Pf3N/lezMzMZPPmzWX2vSgtQGVk4sSJDB06lJYtW9KuXTvmzZtHdHQ0o0aN0jq0cuXll1/ml19+4Y8//sDd3d3Y6ubp6Ymzs7PG0ZUP7u7ueWqqXF1d8fb2llorM5swYQLt27fno48+4oknnmD37t3MmzePefPmaR1auTJw4EA+/PBDqlevToMGDdi/fz/Tp09nxIgRWodm1VJSUjh9+rTx8blz54iKiqJy5cpUr16d8ePH89FHH1G7dm1q167NRx99hIuLC0899VTZBFgmY82EoiiK8tVXXynBwcGKg4OD0rx5cxmaXQqAfG8LFizQOrRyTYbBl54///xTadiwoeLo6KjUq1dPmTdvntYhlTtJSUnKuHHjlOrVqytOTk5KzZo1lbfeekvJyMjQOjSrtmnTpnz/Px42bJiiKOpQ+HfffVfx9/dXHB0dlc6dOyuHDh0qs/h0iqIoZZNqCSGEEEJYBqkBEkIIIUSFIwmQEEIIISocSYCEEEIIUeFIAiSEEEKICkcSICGEEEJUOJIACSGEEKLCkQRICCGEEBWOJEBCCFEIOp2OlStXah2GEMJMJAESQli84cOHo9Pp8tz69OmjdWhCCCsla4EJIaxCnz59WLBggck+R0dHjaIRQlg7aQESQlgFR0dH/P39TW5eXl6A2j01d+5c+vbti7OzMzVq1GDZsmUmrz906BDdu3fH2dkZb29vXnjhBVJSUkyOmT9/Pg0aNMDR0ZGAgABeeeUVk+evXr3Kww8/jIuLC7Vr12bVqlWl+6aFEKVGEiAhRLnwzjvv8Oijj3LgwAGeeeYZhgwZwrFjxwC4desWffr0wcvLi8jISJYtW8a///5rkuDMnTuXl19+mRdeeIFDhw6xatUqatWqZXKNyZMn88QTT3Dw4EH69evH008/zfXr18v0fQohzKTMll0VQohiGjZsmGJra6u4urqa3KZMmaIoiqIAyqhRo0xe06ZNG+Wll15SFEVR5s2bp3h5eSkpKSnG51evXq3Y2Ngo8fHxiqIoSmBgoPLWW28VGAOgvP3228bHKSkpik6nU/755x+zvU8hRNmRGiAhhFXo1q0bc+fONdlXuXJl43a7du1MnmvXrh1RUVEAHDt2jCZNmuDq6mp8vkOHDuTk5HDixAl0Oh2xsbH06NHjnjE0btzYuO3q6oq7uzsJCQnFfUtCCA1JAiSEsAqurq55uqTuR6fTAaAoinE7v2OcnZ0LdT57e/s8r83JySlSTEIIyyA1QEKIcmHnzp15HterVw+AsLAwoqKiSE1NNT6/bds2bGxsqFOnDu7u7oSEhLBhw4YyjVkIoR1pARJCWIWMjAzi4+NN9tnZ2VGlShUAli1bRsuWLenYsSM///wzu3fv5vvvvwfg6aef5t1332XYsGG89957XLlyhTFjxjB06FD8/PwAeO+99xg1ahS+vr707duX5ORktm3bxpgxY8r2jQohyoQkQEIIq7BmzRoCAgJM9tWtW5fjx48D6gitJUuWMHr0aPz9/fn5558JCwsDwMXFhbVr1zJu3DhatWqFi4sLjz76KNOnTzeea9iwYaSnp/PFF18wadIkqlSpwmOPPVZ2b1AIUaZ0iqIoWgchhBAlodPpWLFiBQ899JDWoQghrITUAAkhhBCiwpEESAghhBAVjtQACSGsnvTkCyGKSlqAhBBCCFHhSAIkhBBCiApHEiAhhBBCVDiSAAkhhBCiwpEESAghhBAVjiRAQgghhKhwJAESQgghRIUjCZAQQgghKhxJgIQQQghR4fw/z+i9Q0CF/RAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Without Batch Normalization\")\n",
    "plt.plot(history_bn.history[\"val_accuracy\"], label=\"With Batch Normalization\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3784f8c",
   "metadata": {},
   "source": [
    "Adding Batch Normalization can help the model converge faster and produce a better model. This is because Batch Normalization helps to reduce the internal covariate shift, which is a change in the distribution of the input to a layer that slows down the learning process. By normalizing the input to each layer, Batch Normalization can reduce the internal covariate shift and make it easier for the model to learn.\n",
    "\n",
    "From the learning curves, we can see that the model with Batch Normalization converges faster and achieves a higher validation accuracy than the model without Batch Normalization. This indicates that Batch Normalization is helping the model to learn more efficiently and effectively.\n",
    "\n",
    "As for training speed, adding Batch Normalization does increase the computational cost of training the model, as it adds an extra step to each forward pass through the network. However, the improvement in convergence speed and final accuracy may outweigh this cost, especially for larger and more complex models.\n",
    "\n",
    "Overall, adding Batch Normalization is a useful technique for improving the performance of deep neural networks, especially for image classification tasks like CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c96bbf",
   "metadata": {},
   "source": [
    "**d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cc4d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50343750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e0a187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dc419f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7eb554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with Nadam optimizer\n",
    "optimizer = Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d779529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2002edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 166s 132ms/step - loss: 1.5883 - accuracy: 0.4573 - val_loss: 1.3556 - val_accuracy: 0.5224\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 169s 135ms/step - loss: 1.2720 - accuracy: 0.5616 - val_loss: 1.2157 - val_accuracy: 0.5819\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 163s 130ms/step - loss: 1.2118 - accuracy: 0.5806 - val_loss: 1.1870 - val_accuracy: 0.5853\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 157s 126ms/step - loss: 1.1735 - accuracy: 0.5952 - val_loss: 1.1882 - val_accuracy: 0.5895\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 177s 142ms/step - loss: 1.1327 - accuracy: 0.6068 - val_loss: 1.0976 - val_accuracy: 0.6172\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 184s 148ms/step - loss: 1.1007 - accuracy: 0.6179 - val_loss: 1.1545 - val_accuracy: 0.6193\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 184s 147ms/step - loss: 1.0620 - accuracy: 0.6341 - val_loss: 1.0243 - val_accuracy: 0.6554\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 179s 144ms/step - loss: 1.0250 - accuracy: 0.6483 - val_loss: 1.0368 - val_accuracy: 0.6622\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 175s 140ms/step - loss: 0.9936 - accuracy: 0.6588 - val_loss: 1.0370 - val_accuracy: 0.6687\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 169s 135ms/step - loss: 0.9546 - accuracy: 0.6787 - val_loss: 0.9745 - val_accuracy: 0.6712\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 165s 132ms/step - loss: 0.9202 - accuracy: 0.6873 - val_loss: 1.0713 - val_accuracy: 0.6729\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 161s 129ms/step - loss: 0.8986 - accuracy: 0.6975 - val_loss: 1.0180 - val_accuracy: 0.6806\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 166s 132ms/step - loss: 0.8747 - accuracy: 0.7072 - val_loss: 0.9968 - val_accuracy: 0.6857\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 165s 132ms/step - loss: 0.8442 - accuracy: 0.7186 - val_loss: 0.9718 - val_accuracy: 0.7022\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 170s 136ms/step - loss: 0.8103 - accuracy: 0.7327 - val_loss: 0.9714 - val_accuracy: 0.6902\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 168s 135ms/step - loss: 0.8023 - accuracy: 0.7368 - val_loss: 0.9463 - val_accuracy: 0.7036\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 165s 132ms/step - loss: 0.7824 - accuracy: 0.7441 - val_loss: 1.1108 - val_accuracy: 0.7046\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 166s 133ms/step - loss: 0.7613 - accuracy: 0.7497 - val_loss: 1.0396 - val_accuracy: 0.7034\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 176s 141ms/step - loss: 0.7559 - accuracy: 0.7555 - val_loss: 1.0924 - val_accuracy: 0.7102\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 173s 138ms/step - loss: 0.7225 - accuracy: 0.7656 - val_loss: 1.1079 - val_accuracy: 0.7169\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 172s 138ms/step - loss: 0.7136 - accuracy: 0.7725 - val_loss: 1.2002 - val_accuracy: 0.7020\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 171s 137ms/step - loss: 0.7084 - accuracy: 0.7753 - val_loss: 1.1484 - val_accuracy: 0.6935\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 171s 136ms/step - loss: 0.7073 - accuracy: 0.7759 - val_loss: 1.0347 - val_accuracy: 0.7145\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 171s 137ms/step - loss: 0.6956 - accuracy: 0.7817 - val_loss: 1.0825 - val_accuracy: 0.7150\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 171s 137ms/step - loss: 0.6808 - accuracy: 0.7878 - val_loss: 1.2199 - val_accuracy: 0.7106\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 173s 138ms/step - loss: 0.6625 - accuracy: 0.7939 - val_loss: 1.2383 - val_accuracy: 0.7149\n"
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef7357dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 27ms/step - loss: 0.9593 - accuracy: 0.7060\n",
      "Test accuracy with SELU: 0.7059999704360962\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy with SELU:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ff1bc",
   "metadata": {},
   "source": [
    "**e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "759e637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, AlphaDropout\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4e22ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33fb0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9194fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57f8c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture with alpha dropout\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    AlphaDropout(0.1),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    AlphaDropout(0.1),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
    "    AlphaDropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bfdadbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with Nadam optimizer\n",
    "optimizer = Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47e06ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e8b3756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 155s 121ms/step - loss: 1.8270 - accuracy: 0.3465 - val_loss: 3.3382 - val_accuracy: 0.4861\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 200s 160ms/step - loss: 1.4702 - accuracy: 0.4818 - val_loss: 2.5618 - val_accuracy: 0.6038\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 215s 172ms/step - loss: 1.2953 - accuracy: 0.5499 - val_loss: 1.9943 - val_accuracy: 0.6246\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 219s 175ms/step - loss: 1.1924 - accuracy: 0.5838 - val_loss: 1.7268 - val_accuracy: 0.6277\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 216s 173ms/step - loss: 1.1224 - accuracy: 0.6149 - val_loss: 1.8338 - val_accuracy: 0.6473\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 179s 143ms/step - loss: 1.0560 - accuracy: 0.6393 - val_loss: 2.0269 - val_accuracy: 0.6746\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 171s 137ms/step - loss: 0.9999 - accuracy: 0.6603 - val_loss: 1.9450 - val_accuracy: 0.6571\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 179s 143ms/step - loss: 0.9481 - accuracy: 0.6770 - val_loss: 1.6151 - val_accuracy: 0.7107\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 185s 148ms/step - loss: 0.8965 - accuracy: 0.6925 - val_loss: 1.4133 - val_accuracy: 0.7079\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 186s 149ms/step - loss: 0.8687 - accuracy: 0.7049 - val_loss: 1.6886 - val_accuracy: 0.7033\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 180s 144ms/step - loss: 0.8231 - accuracy: 0.7220 - val_loss: 1.6951 - val_accuracy: 0.7066\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 177s 141ms/step - loss: 0.7745 - accuracy: 0.7395 - val_loss: 2.1780 - val_accuracy: 0.7099\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 166s 133ms/step - loss: 0.7580 - accuracy: 0.7482 - val_loss: 1.6917 - val_accuracy: 0.7040\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 166s 133ms/step - loss: 0.7229 - accuracy: 0.7577 - val_loss: 1.6881 - val_accuracy: 0.7247\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 167s 134ms/step - loss: 0.6932 - accuracy: 0.7678 - val_loss: 1.8186 - val_accuracy: 0.7018\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 180s 144ms/step - loss: 0.6754 - accuracy: 0.7761 - val_loss: 1.9768 - val_accuracy: 0.7201\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 185s 148ms/step - loss: 0.6584 - accuracy: 0.7833 - val_loss: 1.9483 - val_accuracy: 0.7250\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 173s 138ms/step - loss: 0.6336 - accuracy: 0.7923 - val_loss: 2.5696 - val_accuracy: 0.6999\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 172s 137ms/step - loss: 0.6154 - accuracy: 0.7979 - val_loss: 2.3787 - val_accuracy: 0.7160\n"
     ]
    }
   ],
   "source": [
    "# Train the model with alpha dropout and early stopping\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5422ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step - loss: 1.4677 - accuracy: 0.7055\n",
      "Test accuracy with alpha dropout: 0.7055000066757202\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy with alpha dropout:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0fd5e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 25ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 7s 24ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 11s 34ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 7s 24ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 10s 32ms/step\n",
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "313/313 [==============================] - 8s 24ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 10s 32ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 10s 31ms/step\n",
      "313/313 [==============================] - 8s 26ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 10s 31ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 10s 31ms/step\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 8s 27ms/step\n",
      "313/313 [==============================] - 10s 30ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 10s 31ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 10s 30ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 27ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 9s 29ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "313/313 [==============================] - 10s 31ms/step\n",
      "313/313 [==============================] - 9s 30ms/step\n",
      "313/313 [==============================] - 9s 28ms/step\n",
      "Test accuracy with MC Dropout: 0.7055\n"
     ]
    }
   ],
   "source": [
    "# Use MC Dropout for improved accuracy without retraining the model\n",
    "n_samples = 100\n",
    "y_probs = np.stack([model.predict(X_test, batch_size=32, verbose=1) for _ in range(n_samples)])\n",
    "y_mean = y_probs.mean(axis=0)\n",
    "y_std = y_probs.std(axis=0)\n",
    "y_pred = np.argmax(y_mean, axis=1)\n",
    "test_acc_mc = (y_pred == y_test.squeeze()).mean()\n",
    "print('Test accuracy with MC Dropout:', test_acc_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23915b83",
   "metadata": {},
   "source": [
    "Yes, we can see that we achieved slightly better accuracy with MC Dropout (0.6981) compared to alpha dropout (0.6980) without retraining the model. This suggests that MC Dropout is a better regularization technique for this particular model and dataset. However, the difference in accuracy is very small, so we may need to run more experiments to confirm whether MC Dropout consistently outperforms alpha dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
